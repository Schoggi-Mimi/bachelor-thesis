ssim_inference:
  original_path: "../data/test_70"
  distorted_path: "../data/test_70/distorted"
  csv_path: "./csv/ssim_scores.csv"

arniqa_test:
  root: "../data/test_200"
  regressor_dataset: "kadid10k" # or "live", "csiq", "tid2013", "flive", "spaq"
  output_csv: "./csv/quality_scores.csv"

inference:
  model_path: "../model/combined_mlp_reg.pkl"
  images_path: "../data/test_200"
  csv_path: "./csv/predictions.csv"
  batch_size: 10
  num_workers: 1

single_image_inference:
  image_path: "path/to/image.jpg"
  model_path: "../model/combined_mlp_reg.pkl"

test:
  root: "../data/test_70"
  batch_size: 10
  num_workers: 1
  model_path: "../model/combined_mlp_reg.pkl"
  data_type: "s"  # 's' for synthetic, 'a' for authentic

train:
  root: "./data/COMB"
  num_distortions: 64
  batch_size: 10
  num_workers: 1
  model_type: "mlp_reg"  # 'xgb_reg', 'xgb_cls', 'mlp_reg', 'mlp_cls'
  sweep: false  # Set to true to perform hyperparameter sweeping
  sweep_count: 50 
  # Default training parameters
  n_estimators: 100
  max_depth: 6
  learning_rate: 0.01
  subsample: 1.0
  early_stopping_rounds: 10
  gamma: 0
  min_child_weight: 1
  hidden_layer_sizes: [100]
  activation: "relu"
  alpha: 0.0001
  learning_rate_init: 0.001
  max_iter: 200
  multi_strategy: "one_output_per_tree"
  # Sweep configuration
  sweep_config:  
    method: 'random'
    metric: {'name': 'overall_srocc', 'goal': 'maximize'}
    parameters:
      root: {'value': 'COMB'} # 'SCIN', 'F17K'
      model_type: {'values': ['xgb_reg', 'xgb_cls', 'mlp_reg', 'mlp_cls']}
      batch_size: {'values': [10, 32]}
      num_distortions: {'values': [10, 16]}
      n_estimators: {'values': [50, 100, 200, 300]}
      learning_rate: {"min": 0.0001, "max": 0.1}
      min_child_weight: {"min": 1, "max": 150}
      early_stopping_rounds: {"values": [10, 20, 30, 40]}
      subsample: {"values": [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]}
      max_depth: {"values": [3, 5, 7, 9]}
      gamma: {"min": 0.001, "max": 0.5}
      multi_strategy: {"values": ["one_output_per_tree", "multi_output_tree"]}
      hidden_layer_sizes: {"values": [(512,), (1024,), (512, 256), (1024, 512), (512, 512)]}
      activation: {"values": ["relu", "tanh"]}
      alpha: {"min": 0.0001, "max": 0.01}
      learning_rate_init: {"min": 0.0001, "max": 0.1}
      max_iter: {"values": [200, 300, 500]}

logging:
  use_wandb: true   # if True, use wandb for logging
  wandb:
    project: ""   # wandb project name
    entity: ""    # wandb entity name