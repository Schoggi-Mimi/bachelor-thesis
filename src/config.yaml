ssim_inference:
  original_path: "../datas/test_70"
  distorted_path: "../datas/test_70/distorted"
  csv_path: "./csv/ssim_scores.csv"

arniqa_test:
  root: "../datas/test_200"
  regressor_dataset: "kadid10k" # or "live", "csiq", "tid2013", "flive", "spaq"
  output_csv: "./csv/quality_scores.csv"

inference:
  model_path: "../models/combined_mlp_reg.pkl"
  images_path: "../datas/test_200"
  csv_path: "./csv/predictions.csv"
  batch_size: 10
  num_workers: 1

single_image_inference:
  image_path: "../datas/ood/I81.png"
  model_path: "../models/combined_mlp_reg.pkl"

test:
  root: "../datas/test_70"
  batch_size: 10
  num_workers: 1
  model_path: "../models/combined_mlp_reg.pkl"
  data_type: "s"  # 's' for synthetic, 'a' for authentic

train:
  root: "../datas/F17K" # change to COMB
  model_save_path: "../models/final_model.pkl"
  model_save: false
  num_distortions: 1 # chagen to 64
  plot_results: True  # set to false if sweep is true
  batch_size: 10
  num_workers: 1
  model_type: "mlp_reg"  # 'xgb_reg', 'xgb_cls', 'mlp_reg', 'mlp_cls'
  sweep: False  # Set to true to perform hyperparameter sweeping
  sweep_count: 10
  # Default training parameters
  n_estimators: 100
  max_depth: 6
  learning_rate: 0.01
  subsample: 1.0
  early_stopping_rounds: 10
  gamma: 0
  min_child_weight: 1
  hidden_layer_sizes: [512]
  activation: "relu"
  alpha: 0.0001
  learning_rate_init: 0.001
  max_iter: 200
  multi_strategy: "one_output_per_tree"
  # Sweep configuration
  sweep_config:  
    method: 'random'
    metric: {'name': 'overall_srocc', 'goal': 'maximize'}
    parameters:
      root: {'value': '../datas/F17K'} # 'SCIN', 'F17K'
      model_type: {'values': ['xgb_reg', 'xgb_cls', 'mlp_reg', 'mlp_cls']} 
      batch_size: {'value': 10}
      num_distortions: {'values': [10, 1]}
      n_estimators: {'values': [50, 100, 200, 300]}
      learning_rate: {"min": 0.0001, "max": 0.1}
      min_child_weight: {"min": 1, "max": 150}
      early_stopping_rounds: {"values": [10, 20, 30, 40]}
      subsample: {"values": [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]}
      max_depth: {"values": [3, 5, 7, 9]}
      gamma: {"min": 0.001, "max": 0.5}
      multi_strategy: {"values": ["one_output_per_tree", "multi_output_tree"]}
      hidden_layer_sizes: {"values": [[512], [1024], [512, 256], [1024, 512], [512, 512]]}
      activation: {"value": "relu"}
      alpha: {"min": 0.0001, "max": 0.01}
      learning_rate_init: {"min": 0.0001, "max": 0.1}
      max_iter: {"values": [200, 300, 500]}
  logging:
    use_wandb: False   # if True, use wandb for logging
    wandb:
      project: ""   # wandb project name
      entity: ""    # wandb entity name

