{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Data Collection**:\n",
    "   - Gather a diverse set of high-quality dermatology images. Ensure that these images cover a wide range of skin conditions and are representative of the types of images that your assessment system will encounter.\n",
    "   - Create distorted versions of these images to simulate various quality criteria such as lighting, background, field of view, orientation, focus & depth of field, resolution, and color calibration. You can use image editing software or libraries like OpenCV to introduce these distortions.\n",
    "\n",
    "2. **Preprocessing**:\n",
    "   - Resize all images to a consistent resolution to ensure uniformity in processing.\n",
    "   - Normalize pixel values to a common scale (e.g., [0, 1]) to improve model convergence.\n",
    "\n",
    "3. **Feature Extraction**:\n",
    "   - Extract relevant features from the images. For example, you can use techniques like Histogram of Oriented Gradients (HOG), Local Binary Patterns (LBP), or deep learning-based feature extraction methods.\n",
    "\n",
    "4. **Model Building**:\n",
    "   - Design and train a machine learning model to assess image quality based on the extracted features.\n",
    "   - Choose an appropriate model architecture based on the complexity of the task and the amount of data available. Common choices include Support Vector Machines (SVM), Random Forests, Convolutional Neural Networks (CNNs), or transfer learning from pre-trained models like VGG, ResNet, or EfficientNet.\n",
    "   - Split your dataset into training, validation, and test sets to evaluate the performance of your model accurately.\n",
    "\n",
    "5. **Evaluation**:\n",
    "   - Evaluate the performance of your model using appropriate metrics such as accuracy, precision, recall, F1-score, or area under the ROC curve (AUC).\n",
    "   - Use techniques like cross-validation to ensure the robustness of your model's performance.\n",
    "\n",
    "6. **Fine-tuning and Optimization**:\n",
    "   - Fine-tune your model based on the evaluation results. Adjust hyperparameters, try different model architectures, or experiment with different feature extraction techniques to improve performance.\n",
    "   - Consider techniques like data augmentation to increase the diversity of your training data and improve generalization.\n",
    "\n",
    "7. **Challenges**:\n",
    "   - **Data Quality**: Ensuring that your training data accurately represents the diversity of images encountered in teledermatology can be challenging.\n",
    "   - **Subjectivity**: Image quality assessment can be subjective, and different evaluators may have varying opinions on what constitutes a \"good\" or \"bad\" image.\n",
    "   - **Generalization**: Ensuring that your model can generalize well to unseen images is crucial, especially when dealing with real-world applications where image quality may vary significantly.\n",
    "   - **Computational Resources**: Training deep learning models can be computationally intensive, so you may need access to sufficient computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.utils_data import create_distortions_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Images to Distort:   0%|          | 0/410 [00:00<?, ?it/s]/Users/choekyelnyungmartsang/opt/anaconda3/envs/ARNIQA/lib/python3.10/site-packages/torchvision/transforms/functional.py:685: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  b_matrix = torch.tensor(startpoints, dtype=torch.float).view(8)\n",
      "Images to Distort:   0%|          | 2/410 [00:20<1:08:48, 10.12s/it]\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"images\"\n",
    "files = [f for f in os.listdir(folder_path) if not f.startswith('.') and '__' not in f]\n",
    "\n",
    "with tqdm(total=len(files), desc=\"Images to Distort\") as pbar:\n",
    "    for i, f in enumerate(files):\n",
    "        img_path = os.path.join(folder_path, f)\n",
    "        create_distortions_batch(img_path, folder_path, batch_size=32, counter=i)\n",
    "        pbar.update(1)\n",
    "        if i == 1:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1080, 810])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.utils_data import resize_crop, center_corners_crop\n",
    "\n",
    "folder_path = \"images\"\n",
    "files = [f for f in os.listdir(folder_path) if not f.startswith('.') and '__' not in f]\n",
    "\n",
    "# normal crop\n",
    "test_path = os.path.join(folder_path, files[1])\n",
    "transform = transforms.Compose([\n",
    "    #transforms.Resize(256),                      # Resize the image to 256x256\n",
    "    #transforms.CenterCrop(224),                  # Center crop the image to 224x224\n",
    "    transforms.ToTensor(),                       # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image\n",
    "])\n",
    "orig_image = Image.open(test_path).convert('RGB')\n",
    "image = transform(orig_image)\n",
    "#image = transform(image).unsqueeze(0)\n",
    "\n",
    "#Â center_corners_crop\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "img1 = Image.open(test_path).convert('RGB')\n",
    "img1 = center_corners_crop(img1, crop_size=224)\n",
    "img1 = [transforms.ToTensor()(crop) for crop in img1]\n",
    "img1 = torch.stack(img1, dim=0)\n",
    "im1g = normalize(img1)\n",
    "\n",
    "# resize_crop\n",
    "img2 = Image.open(test_path).convert('RGB')\n",
    "img2 = resize_crop(img2, crop_size=224)\n",
    "img2 = transforms.ToTensor()(img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(orig_img, crop_img, distort_img):\n",
    "    _, axes = plt.subplots(1, 3, figsize=(15, 5))  # Create a figure with three subplots\n",
    "    axes[0].imshow(orig_img)\n",
    "    axes[0].set_title('Original Image')\n",
    "    axes[0].axis('off')\n",
    "    axes[1].imshow(crop_img.permute(1, 2, 0).cpu().numpy().clip(0, 1))\n",
    "    axes[1].set_title('Normalized Image')\n",
    "    axes[1].axis('off')\n",
    "    axes[2].imshow(distort_img.permute(1, 2, 0).cpu().numpy())\n",
    "    axes[2].set_title('Distorted Image')\n",
    "    axes[2].axis('off')\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout to prevent overlap of subplots\n",
    "    plt.show()\n",
    "display(orig_image, image, img_A_orig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ARNIQA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
