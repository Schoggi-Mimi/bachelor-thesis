@article{A57,
  title = {{{VSNR}}: {{A Wavelet-Based Visual Signal-to-Noise Ratio}} for {{Natural Images}}},
  shorttitle = {{{VSNR}}},
  author = {Chandler, D.M. and Hemami, S.S.},
  date = {2007-09},
  journaltitle = {IEEE Transactions on Image Processing},
  shortjournal = {IEEE Trans. on Image Process.},
  volume = {16},
  number = {9},
  pages = {2284--2298},
  issn = {1057-7149},
  doi = {10.1109/TIP.2007.901820},
  url = {http://ieeexplore.ieee.org/document/4286985/},
  urldate = {2024-04-23},
  abstract = {This paper presents an efficient metric for quantifying the visual fidelity of natural images based on near-threshold and suprathreshold properties of human vision. The proposed metric, the visual signal-to-noise ratio (VSNR), operates via a two-stage approach. In the first stage, contrast thresholds for detection of distortions in the presence of natural images are computed via wavelet-based models of visual masking and visual summation in order to determine whether the distortions in the distorted image are visible. If the distortions are below the threshold of detection, the distorted image is deemed to be of perfect visual fidelity (VSNR = ) and no further analysis is required. If the distortions are suprathreshold, a second stage is applied which operates based on the low-level visual property of perceived contrast, and the mid-level visual property of global precedence. These two properties are modeled as Euclidean distances in distortion-contrast space of a multiscale wavelet decomposition, and VSNR is computed based on a simple linear sum of these distances. The proposed VSNR metric is generally competitive with current metrics of visual fidelity; it is efficient both in terms of its low computational complexity and in terms of its low memory requirements; and it operates based on physical luminances and visual angle (rather than on digital pixel values and pixel-based dimensions) to accommodate different viewing conditions.},
  langid = {english},
  file = {/Users/choekyelnyungmartsang/Zotero/storage/EW9CANZ4/Chandler und Hemami - 2007 - VSNR A Wavelet-Based Visual Signal-to-Noise Ratio.pdf}
}

@article{CCT,
  title = {Unified {{Blind Quality Assessment}} of {{Compressed Natural}}, {{Graphic}}, and {{Screen Content Images}}},
  author = {Min, Xiongkuo and Ma, Kede and Gu, Ke and Zhai, Guangtao and Wang, Zhou and Lin, Weisi},
  date = {2017-11},
  journaltitle = {IEEE Transactions on Image Processing},
  shortjournal = {IEEE Trans. on Image Process.},
  volume = {26},
  number = {11},
  pages = {5462--5474},
  issn = {1057-7149, 1941-0042},
  doi = {10.1109/TIP.2017.2735192},
  url = {http://ieeexplore.ieee.org/document/8000398/},
  urldate = {2024-04-23},
  abstract = {Digital images in the real world are created by a variety of means and have diverse properties. A photographical natural scene image (NSI) may exhibit substantially different characteristics from a computer graphic image (CGI) or a screen content image (SCI). This casts major challenges to objective image quality assessment, for which existing approaches lack effective mechanisms to capture such content type variations, and thus are difficult to generalize from one type to another. To tackle this problem, we first construct a cross-content-type (CCT) database, which contains 1,320 distorted NSIs, CGIs, and SCIs, compressed using the high efficiency video coding (HEVC) intra coding method and the screen content compression (SCC) extension of HEVC. We then carry out a subjective experiment on the database in a well-controlled laboratory environment. Moreover, we propose a unified content-type adaptive (UCA) blind image quality assessment model that is applicable across content types. A key step in UCA is to incorporate the variations of human perceptual characteristics in viewing different content types through a multi-scale weighting framework. This leads to superior performance on the constructed CCT database. UCA is training-free, implying strong generalizability. To verify this, we test UCA on other databases containing JPEG, MPEG-2, H.264, and HEVC compressed images/videos, and observe that it consistently achieves competitive performance.},
  langid = {english},
  file = {/Users/choekyelnyungmartsang/Zotero/storage/FCJP6NPM/Min et al. - 2017 - Unified Blind Quality Assessment of Compressed Nat.pdf}
}

@article{CID2013,
  title = {{{CID2013}}: {{A Database}} for {{Evaluating No-Reference Image Quality Assessment Algorithms}}},
  shorttitle = {{{CID2013}}},
  author = {Virtanen, Toni and Nuutinen, Mikko and Vaahteranoksa, Mikko and Oittinen, Pirkko and Hakkinen, Jukka},
  date = {2015-01},
  journaltitle = {IEEE Transactions on Image Processing},
  shortjournal = {IEEE Trans. on Image Process.},
  volume = {24},
  number = {1},
  pages = {390--402},
  issn = {1057-7149, 1941-0042},
  doi = {10.1109/TIP.2014.2378061},
  url = {http://ieeexplore.ieee.org/document/6975172/},
  urldate = {2024-04-23},
  abstract = {This paper presents a new database, CID2013, to address the issue of using no-reference (NR) image quality assessment algorithms on images with multiple distortions. Current NR algorithms struggle to handle images with many concurrent distortion types, such as real photographic images captured by different digital cameras. The database consists of six image sets; on average, 30 subjects have evaluated 12–14 devices depicting eight different scenes for a total of 79 different cameras, 480 images, and 188 subjects (67\% female). The subjective evaluation method was a hybrid absolute category rating-pair comparison developed for the study and presented in this paper. This method utilizes a slideshow of all images within a scene to allow the test images to work as references to each other. In addition to mean opinion score value, the images are also rated using sharpness, graininess, lightness, and color saturation scales. The CID2013 database contains images used in the experiments with the full subjective data plus extensive background information from the subjects. The database is made freely available for the research community.},
  langid = {english},
  file = {/Users/choekyelnyungmartsang/Zotero/storage/KHXSACRZ/Virtanen et al. - 2015 - CID2013 A Database for Evaluating No-Reference Im.pdf}
}

@article{CSIQ,
  title = {Most Apparent Distortion: Full-Reference Image Quality Assessment and the Role of Strategy},
  shorttitle = {Most Apparent Distortion},
  author = {Chandler, Damon M.},
  date = {2010-01-01},
  journaltitle = {Journal of Electronic Imaging},
  shortjournal = {J. Electron. Imaging},
  volume = {19},
  number = {1},
  pages = {011006},
  issn = {1017-9909},
  doi = {10.1117/1.3267105},
  url = {https://s2.smu.edu/~eclarson/pubs/2010JEI_MAD.pdf},
  urldate = {2024-04-23},
  langid = {english},
  file = {/Users/choekyelnyungmartsang/Zotero/storage/ND7PFFNY/Chandler - 2010 - Most apparent distortion full-reference image qua.pdf}
}

@article{HSNID,
  title = {Learning a {{Unified Blind Image Quality Metric}} via {{On-Line}} and {{Off-Line Big Training Instances}}},
  author = {Gu, Ke and Xu, Xin and Qiao, Junfei and Jiang, Qiuping and Lin, Weisi and Thalmann, Daniel},
  date = {2020-12-01},
  journaltitle = {IEEE Transactions on Big Data},
  shortjournal = {IEEE Trans. Big Data},
  volume = {6},
  number = {4},
  pages = {780--791},
  issn = {2332-7790, 2372-2096},
  doi = {10.1109/TBDATA.2019.2895605},
  url = {https://ieeexplore.ieee.org/document/8627983/},
  urldate = {2024-04-23},
  abstract = {In this work, we resolve a big challenge that most current image quality metrics (IQMs) are unavailable across different image contents, especially simultaneously coping with natural scene (NS) images or screen content (SC) images. By comparison with existing works, this paper deploys on-line and off-line data for proposing a unified no-reference (NR) IQM, not only applied to different distortion types and intensities but also to various image contents including classical NS images and prevailing SC images. Our proposed NR IQM is developed with two data-driven learning processes following feature extraction, which is based on scene statistic models, free-energy brain principle, and human visual system (HVS) characteristics. In the first process, the scene statistic models and an image retrieve technique are combined, based on on-line and off-line training instances, to derive a novel loose classifier for retrieving clean images and helping to infer the image content. In the second process, the features extracted by incorporating the inferred image content, free-energy and low-level perceptual characteristics of the HVS are learned by utilizing off-line training samples to analyze the distortion types and intensities and thereby to predict the image quality. The two processes mentioned above depend on a gigantic quantity of training data, much exceeding the number of images applied to performance validation, and thus make our model’s performance more reliable. Through extensive experiments, it has been validated that the proposed blind IQM is capable of simultaneously inferring the quality of NS and SC images, and it has attained superior performance as compared with popular and state-of-the-art IQMs on the subjective NS and SC image quality databases. The source code of our model will be released with the publication of the paper at https://kegu.netlify.com.},
  langid = {english},
  file = {/Users/choekyelnyungmartsang/Zotero/storage/8K778KAQ/Gu et al. - 2020 - Learning a Unified Blind Image Quality Metric via .pdf}
}

@online{LIVE,
  title = {Laboratory for {{Image}} and {{Video Engineering}} - {{The University}} of {{Texas}} at {{Austin}}},
  author = {Sheikh, Hamid Rahim and Bovik, Alan C. and Cormack, Lawrence and Wang, Zhou},
  url = {https://live.ece.utexas.edu/research/quality/subjective.htm},
  urldate = {2024-04-23},
  organization = {LIVE Image Quality Assessment Database release 2},
  file = {/Users/choekyelnyungmartsang/Zotero/storage/FSBIR66A/subjective.html}
}

@article{LIVE_Wild,
  title = {Massive {{Online Crowdsourced Study}} of {{Subjective}} and {{Objective Picture Quality}}},
  author = {Ghadiyaram, Deepti and Bovik, Alan C.},
  date = {2016-01},
  journaltitle = {IEEE Transactions on Image Processing},
  shortjournal = {IEEE Trans. on Image Process.},
  volume = {25},
  number = {1},
  pages = {372--387},
  issn = {1057-7149, 1941-0042},
  doi = {10.1109/TIP.2015.2500021},
  url = {http://ieeexplore.ieee.org/document/7327186/},
  urldate = {2024-04-23},
  abstract = {Most publicly available image quality databases have been created under highly controlled conditions by introducing graded simulated distortions onto high-quality photographs. However, images captured using typical real-world mobile camera devices are usually afflicted by complex mixtures of multiple distortions, which are not necessarily well-modeled by the synthetic distortions found in existing databases. The originators of existing legacy databases usually conducted human psychometric studies to obtain statistically meaningful sets of human opinion scores on images in a stringently controlled visual environment, resulting in small data collections relative to other kinds of image analysis databases. Toward overcoming these limitations, we designed and created a new database that we call the LIVE In the Wild Image Quality Challenge Database, which contains widely diverse authentic image distortions on a large number of images captured using a representative variety of modern mobile devices. We also designed and implemented a new online crowdsourcing system, which we have used to conduct a very large-scale, multi-month image quality assessment (IQA) subjective study. Our database consists of over 350 000 opinion scores on 1162 images evaluated by over 8100 unique human observers. Despite the lack of control over the experimental environments of the numerous study participants, we demonstrate excellent internal consistency of the subjective data set. We also evaluate several top-performing blind IQA algorithms on it and present insights on how the mixtures of distortions challenge both end users as well as automatic perceptual quality prediction models. The new database is available for public use at http://live.ece.utexas.edu/research/ChallengeDB/index.html.},
  langid = {english},
  file = {/Users/choekyelnyungmartsang/Zotero/storage/NQWGF6QQ/Ghadiyaram und Bovik - 2016 - Massive Online Crowdsourced Study of Subjective an.pdf}
}

@inproceedings{LIVEMD,
  title = {Objective Quality Assessment of Multiply Distorted Images},
  booktitle = {2012 {{Conference Record}} of the {{Forty Sixth Asilomar Conference}} on {{Signals}}, {{Systems}} and {{Computers}} ({{ASILOMAR}})},
  author = {Jayaraman, Dinesh and Mittal, Anish and Moorthy, Anush K. and Bovik, Alan C.},
  date = {2012-11},
  pages = {1693--1697},
  publisher = {IEEE},
  location = {Pacific Grove, CA, USA},
  doi = {10.1109/ACSSC.2012.6489321},
  url = {http://ieeexplore.ieee.org/document/6489321/},
  urldate = {2024-04-23},
  abstract = {Subjective studies have been conducted in the past to obtain human judgments of visual quality on distorted images in or­ der, among other things, to benchmark objective image qual­ ity assessment (lQA) algorithms. Existing subjective studies primarily have records of human ratings on images that were corrupted by only one of many possible distortions. However, the majority of images that are available for consumption are corrupted by multiple distortions. Towards broadening the corpora of records of human responses to visual distortions, we recently conducted a study on two types of multiply dis­ torted images to obtain human judgments of the visual qual­ ity of such images. Further, we compared the performance of several existing objective image quality measures on the new database and analyze the effects of multiple distortions on commonly used quality-determinant features and on hu­ man ratings.},
  eventtitle = {2012 46th {{Asilomar Conference}} on {{Signals}}, {{Systems}} and {{Computers}}},
  isbn = {978-1-4673-5051-8 978-1-4673-5050-1 978-1-4673-5049-5},
  langid = {english},
  file = {/Users/choekyelnyungmartsang/Zotero/storage/XIL22H3U/Jayaraman et al. - 2012 - Objective quality assessment of multiply distorted.pdf}
}

@article{MDID2013,
  title = {Hybrid {{No-Reference Quality Metric}} for {{Singly}} and {{Multiply Distorted Images}}},
  author = {Gu, Ke and Zhai, Guangtao and Yang, Xiaokang and Zhang, Wenjun},
  date = {2014-09},
  journaltitle = {IEEE Transactions on Broadcasting},
  shortjournal = {IEEE Trans. on Broadcast.},
  volume = {60},
  number = {3},
  pages = {555--567},
  issn = {0018-9316, 1557-9611},
  doi = {10.1109/TBC.2014.2344471},
  url = {https://ieeexplore.ieee.org/document/6879255/},
  urldate = {2024-04-23},
  abstract = {In a typical image communication system, the visual signal presented to the end users may undergo the steps of acquisition, compression and transmission which cause the artifacts of blurring, quantization and noise. However, the researches of image quality assessment (IQA) with multiple distortion types are very limited. In this paper, we first introduce a new multiply distorted image database (MDID2013), which is composed of 324 images that are simultaneously corrupted by blurring, JPEG compression and noise injection. We then propose a new six-step blind metric (SISBLIM) for quality assessment of both singly and multiply distorted images. Inspired by the early human visual model and recently revealed free energy based brain theory, our method works to systematically combine the single quality prediction of each emerging distortion type and joint effects of different distortion sources. Comparative studies of the proposed SISBLIM with popular full-reference IQA approaches and start-of-the-art no-reference IQA metrics are conducted on five singly distorted image databases (LIVE, TID2008, CSIQ, IVC, Toyama) and two newly released multiply distorted image databases (LIVEMD, MDID2013). Experimental results confirm the effectiveness of our blind technique. MATLAB codes of the proposed SISBLIM algorithm and MDID2013 database will be available online at http://gvsp.sjtu.edu.cn/.},
  langid = {english},
  file = {/Users/choekyelnyungmartsang/Zotero/storage/B4SN597A/Gu et al. - 2014 - Hybrid No-Reference Quality Metric for Singly and .pdf}
}

@article{MDID2016,
  title = {{{MDID}}: {{A}} Multiply Distorted Image Database for Image Quality Assessment},
  shorttitle = {{{MDID}}},
  author = {Sun, Wen and Zhou, Fei and Liao, Qingmin},
  date = {2017-01},
  journaltitle = {Pattern Recognition},
  shortjournal = {Pattern Recognition},
  volume = {61},
  pages = {153--168},
  issn = {00313203},
  doi = {10.1016/j.patcog.2016.07.033},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320316301911},
  urldate = {2024-04-23},
  abstract = {In this paper, we present a new database, the multiply distorted image database (MDID), to evaluate image quality assessment (IQA) metrics on multiply distorted images. The database contains 20 reference images and 1600 distorted images. The latter images are obtained by contamination of the former with multiple distortions of random types and levels, so multiple types of distortions appear in each distorted image. Pair comparison sorting (PCS) is used as a new subjective rating method to evaluate image quality. This method allows subjects to make equal decisions on images whose difference in quality cannot be easily evaluated visually. A total of 192 subjects participated in the subjective rating, in which mean opinion scores and standard deviations were obtained. In IQA research, subjective scores and algorithm predictions are generally related by a nonlinear regression. We further propose a method to initialize the parameters of the nonlinear regression. The experiments of IQA metrics conducted on MDID validate that this database is advisable and challenging.},
  langid = {english},
  file = {/Users/choekyelnyungmartsang/Zotero/storage/JY7KJWKT/Sun et al. - 2017 - MDID A multiply distorted image database for imag.pdf}
}

@online{SCIN,
  title = {Crowdsourcing {{Dermatology Images}} with {{Google Search Ads}}: {{Creating}} a {{Real-World Skin Condition Dataset}}},
  shorttitle = {Crowdsourcing {{Dermatology Images}} with {{Google Search Ads}}},
  author = {Ward, Abbi and Li, Jimmy and Wang, Julie and Lakshminarasimhan, Sriram and Carrick, Ashley and Campana, Bilson and Hartford, Jay and S, Pradeep Kumar and Tiyasirichokchai, Tiya and Virmani, Sunny and Wong, Renee and Matias, Yossi and Corrado, Greg S. and Webster, Dale R. and Siegel, Dawn and Lin, Steven and Ko, Justin and Karthikesalingam, Alan and Semturs, Christopher and Rao, Pooja},
  date = {2024-02-28},
  eprint = {2402.18545},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2402.18545},
  urldate = {2024-04-23},
  abstract = {Background: Health datasets from clinical sources do not reflect the breadth and diversity of disease in the real world, impacting research, medical education, and artificial intelligence (AI) tool development. Dermatology is a suitable area to develop and test a new and scalable method to create representative health datasets. Methods: We used Google Search advertisements to invite contributions to an open access dataset of images of dermatology conditions, demographic, and symptom information. With informed contributor consent, we describe and release this dataset containing 10,408 images from 5,033 contributions from internet users in the United States over 8 months starting March 2023. The dataset includes dermatologist condition labels as well as estimated Fitzpatrick Skin Type (eFST) and Monk Skin Tone (eMST) labels for the images. Results: We received a median of 22 submissions/day (IQR 14–30). Female (66.72\%) and younger (52\% {$<$} age 40) contributors had a higher representation in the dataset compared to the US population, and 32.6\% of contributors reported a non-White racial or ethnic identity. Over 97.5\% of contributions were genuine images of skin conditions. Dermatologist confidence in assigning a differential diagnosis increased with the number of available variables, and showed a weak correlation with image sharpness (Spearman’s P values {$<$} 0.001 and 0.01 respectively). Most contributions were short-duration (54\% with onset {$<$} 7 days ago ) and 89\% were allergic, infectious, or inflammatory conditions. eFST and eMST distributions reflected the geographical origin of the dataset. The dataset is available at github.com/google-researchdatasets/scin. Conclusion: Search ads are effective at crowdsourcing images of health conditions. The SCIN dataset bridges important gaps in the availability of representative images of common skin conditions.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computers and Society},
  file = {/Users/choekyelnyungmartsang/Zotero/storage/CAC4SASZ/Ward et al. - 2024 - Crowdsourcing Dermatology Images with Google Searc.pdf}
}

@article{SCIQ,
  title = {{{ESIM}}: {{Edge Similarity}} for {{Screen Content Image Quality Assessment}}},
  shorttitle = {{{ESIM}}},
  author = {Ni, Zhangkai and Ma, Lin and Zeng, Huanqiang and Chen, Jing and Cai, Canhui and Ma, Kai-Kuang},
  date = {2017-10},
  journaltitle = {IEEE Transactions on Image Processing},
  shortjournal = {IEEE Trans. on Image Process.},
  volume = {26},
  number = {10},
  pages = {4818--4831},
  issn = {1057-7149, 1941-0042},
  doi = {10.1109/TIP.2017.2718185},
  url = {http://ieeexplore.ieee.org/document/7954714/},
  urldate = {2024-04-23},
  abstract = {In this paper, an accurate full-reference image quality assessment (IQA) model developed for assessing screen content images (SCIs), called the edge similarity (ESIM), is proposed. It is inspired by the fact that the human visual system (HVS) is highly sensitive to edges that are often encountered in SCIs; therefore, essential edge features are extracted and exploited for conducting IQA for the SCIs. The key novelty of the proposed ESIM lies in the extraction and use of three salient edge features—i.e., edge contrast, edge width, and edge direction. The first two attributes are simultaneously generated from the input SCI based on a parametric edge model, while the last one is derived directly from the input SCI. The extraction of these three features will be performed for the reference SCI and the distorted SCI, individually. The degree of similarity measured for each above-mentioned edge attribute is then computed independently, followed by combining them together using our proposed edge-width pooling strategy to generate the final ESIM score. To conduct the performance evaluation of our proposed ESIM model, a new and the largest SCI database (denoted as SCID) is established in our work and made to the public for download. Our database contains 1800 distorted SCIs that are generated from 40 reference SCIs. For each SCI, nine distortion types are investigated, and five degradation levels are produced for each distortion type. Extensive simulation results have clearly shown that the proposed ESIM model is more consistent with the perception of the HVS on the evaluation of distorted SCIs than the multiple state-of-the-art IQA methods.},
  langid = {english},
  file = {/Users/choekyelnyungmartsang/Zotero/storage/KUNRFCHU/Ni et al. - 2017 - ESIM Edge Similarity for Screen Content Image Qua.pdf}
}

@inproceedings{SIQAD,
  title = {Subjective Quality Assessment of {{Screen Content Images}}},
  booktitle = {2014 {{Sixth International Workshop}} on {{Quality}} of {{Multimedia Experience}} ({{QoMEX}})},
  author = {Yang, Huan and {Yuming Fang} and Lin, Weisi and Wang, Zhou},
  date = {2014-09},
  pages = {257--262},
  publisher = {IEEE},
  location = {Singapore, Singapore},
  doi = {10.1109/QoMEX.2014.6982328},
  url = {http://ieeexplore.ieee.org/document/6982328/},
  urldate = {2024-04-23},
  abstract = {Research on Screen Content Images (SCIs) becomes important as they are increasingly used in multi-device communication applications. In this paper, we present a study of subjective quality assessment for distorted SCIs, and investigate which part (text or picture) contributes more to the overall visual quality. We construct a large-scale Screen Image Quality Assessment Database (SIQAD) consisting of 20 source and 980 distorted SCIs. The 11-category Absolute Category Rating (ACR) is employed to obtain three subjective quality scores corresponding to the entire image, textual and pictorial regions respectively. Based on the subjective data, we investigate the applicability of 12 state-of-the-art Image Quality Assessment (IQA) methods for objectively assessing the quality of SCIs. The results indicate that existing IQA methods are limited in predicting human quality judgement of SCIs. Moreover, we propose a prediction model to account for the correlation between the subjective scores of textual and pictorial regions and the entire image. The current results make an initial move towards objective quality assessment of SCIs.},
  eventtitle = {2014 {{Sixth International Workshop}} on {{Quality}} of {{Multimedia Experience}} ({{QoMEX}})},
  isbn = {978-1-4799-6536-6},
  langid = {english},
  file = {/Users/choekyelnyungmartsang/Zotero/storage/46AJMFUP/Yang et al. - 2014 - Subjective quality assessment of Screen Content Im.pdf}
}

@article{TID2008,
  title = {{{TID2008}} – {{A Database}} for {{Evaluation}} of {{Full- Reference Visual Quality Assessment Metrics}}},
  author = {Ponomarenko, Nikolay and Lukin, Vladimir and Zelensky, Alexander and Egiazarian, Karen and Astola, Jaakko and Carli, Marco and Battisti, Federica},
  abstract = {In this paper, a new image database, TID2008, for evaluation of full-reference visual quality assessment metrics is described. It contains 1700 test images (25 reference images, 17 types of distortions for each reference image, 4 different levels of each type of distortion). Mean Opinion Scores (MOS) for this database have been obtained as a result of more than 800 experiments. During these tests, observers from three countries (Finland, Italy, and Ukraine) have carried out about 256000 individual human quality judgments. The obtained MOS can be used for effective testing of different visual quality metrics as well as for the design of new metrics. Using the designed image database, we have tested several known quality metrics. The designed test image database is freely available for downloading and utilization in scientific investigations.},
  langid = {english},
  file = {/Users/choekyelnyungmartsang/Zotero/storage/M93CPMPI/Ponomarenko et al. - TID2008 – A Database for Evaluation of Full- Refer.pdf}
}

@article{TID2013,
  title = {Image Database {{TID2013}}: {{Peculiarities}}, Results and Perspectives},
  shorttitle = {Image Database {{TID2013}}},
  author = {Ponomarenko, Nikolay and Jin, Lina and Ieremeiev, Oleg and Lukin, Vladimir and Egiazarian, Karen and Astola, Jaakko and Vozel, Benoit and Chehdi, Kacem and Carli, Marco and Battisti, Federica and Jay Kuo, C.-C.},
  date = {2015-01},
  journaltitle = {Signal Processing: Image Communication},
  shortjournal = {Signal Processing: Image Communication},
  volume = {30},
  pages = {57--77},
  issn = {09235965},
  doi = {10.1016/j.image.2014.10.009},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0923596514001490},
  urldate = {2024-04-23},
  abstract = {This paper describes a recently created image database, TID2013, intended for evaluation of full-reference visual quality assessment metrics. With respect to TID2008, the new database contains a larger number (3000) of test images obtained from 25 reference images, 24 types of distortions for each reference image, and 5 levels for each type of distortion. Motivations for introducing 7 new types of distortions and one additional level of distortions are given; examples of distorted images are presented. Mean opinion scores (MOS) for the new database have been collected by performing 985 subjective experiments with volunteers (observers) from five countries (Finland, France, Italy, Ukraine, and USA). The availability of MOS allows the use of the designed database as a fundamental tool for assessing the effectiveness of visual quality. Furthermore, existing visual quality metrics have been tested with the proposed database and the collected results have been analyzed using rank order correlation coefficients between MOS and considered metrics. These correlation indices have been obtained both considering the full set of distorted images and specific image subsets, for highlighting advantages and drawbacks of existing, state of the art, quality metrics. Approaches to thorough performance analysis for a given metric are presented to detect practical situations or distortion types for which this metric is not adequate enough to human perception. The created image database and the collected MOS values are freely available for downloading and utilization for scientific purposes.},
  langid = {english},
  file = {/Users/choekyelnyungmartsang/Zotero/storage/NSWLLPSB/Ponomarenko et al. - 2015 - Image database TID2013 Peculiarities, results and.pdf}
}

@article{WED,
  title = {Waterloo {{Exploration Database}}: {{New Challenges}} for {{Image Quality Assessment Models}}},
  shorttitle = {Waterloo {{Exploration Database}}},
  author = {Ma, Kede and Duanmu, Zhengfang and Wu, Qingbo and Wang, Zhou and Yong, Hongwei and Li, Hongliang and Zhang, Lei},
  date = {2017-02},
  journaltitle = {IEEE Transactions on Image Processing},
  shortjournal = {IEEE Trans. on Image Process.},
  volume = {26},
  number = {2},
  pages = {1004--1016},
  issn = {1057-7149, 1941-0042},
  doi = {10.1109/TIP.2016.2631888},
  url = {http://ieeexplore.ieee.org/document/7752930/},
  urldate = {2024-04-23},
  abstract = {The great content diversity of real-world digital images poses a grand challenge to image quality assessment (IQA) models, which are traditionally designed and validated on a handful of commonly used IQA databases with very limited content variation. To test the generalization capability and to facilitate the wide usage of IQA techniques in real-world applications, we establish a large-scale database named the Waterloo Exploration Database, which in its current state contains 4744 pristine natural images and 94 880 distorted images created from them. Instead of collecting the mean opinion score for each image via subjective testing, which is extremely difficult if not impossible, we present three alternative test criteria to evaluate the performance of IQA models, namely, the pristine/distorted image discriminability test, the listwise ranking consistency test, and the pairwise preference consistency test (P-test). We compare 20 well-known IQA models using the proposed criteria, which not only provide a stronger test in a more challenging testing environment for existing models, but also demonstrate the additional benefits of using the proposed database. For example, in the P-test, even for the best performing no-reference IQA model, more than 6 million failure cases against the model are “discovered” automatically out of over 1 billion test pairs. Furthermore, we discuss how the new database may be exploited using innovative approaches in the future, to reveal the weaknesses of existing IQA models, to provide insights on how to improve the models, and to shed light on how the next-generation IQA models may be developed. The database and codes are made publicly available at: https://ece.uwaterloo.ca/\textasciitilde k29ma/exploration/.},
  langid = {english},
  file = {/Users/choekyelnyungmartsang/Zotero/storage/CZGVWNWD/Ma et al. - 2017 - Waterloo Exploration Database New Challenges for .pdf}
}
