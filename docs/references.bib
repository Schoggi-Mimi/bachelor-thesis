@article{A57,
  title        = {{{VSNR}}: {{A Wavelet-Based Visual Signal-to-Noise Ratio}} for {{Natural Images}}},
  shorttitle   = {{{VSNR}}},
  author       = {Chandler, D.M. and Hemami, S.S.},
  date         = {2007-09},
  journaltitle = {IEEE Transactions on Image Processing},
  shortjournal = {IEEE Trans. on Image Process.},
  volume       = {16},
  number       = {9},
  pages        = {2284--2298},
  issn         = {1057-7149},
  doi          = {10.1109/TIP.2007.901820},
  url          = {http://ieeexplore.ieee.org/document/4286985/},
  urldate      = {2024-04-23},
  abstract     = {This paper presents an efficient metric for quantifying the visual fidelity of natural images based on near-threshold and suprathreshold properties of human vision. The proposed metric, the visual signal-to-noise ratio (VSNR), operates via a two-stage approach. In the first stage, contrast thresholds for detection of distortions in the presence of natural images are computed via wavelet-based models of visual masking and visual summation in order to determine whether the distortions in the distorted image are visible. If the distortions are below the threshold of detection, the distorted image is deemed to be of perfect visual fidelity (VSNR = ) and no further analysis is required. If the distortions are suprathreshold, a second stage is applied which operates based on the low-level visual property of perceived contrast, and the mid-level visual property of global precedence. These two properties are modeled as Euclidean distances in distortion-contrast space of a multiscale wavelet decomposition, and VSNR is computed based on a simple linear sum of these distances. The proposed VSNR metric is generally competitive with current metrics of visual fidelity; it is efficient both in terms of its low computational complexity and in terms of its low memory requirements; and it operates based on physical luminances and visual angle (rather than on digital pixel values and pixel-based dimensions) to accommodate different viewing conditions.},
  langid       = {english},
  file         = {/Users/choekyelnyungmartsang/Zotero/storage/EW9CANZ4/Chandler und Hemami - 2007 - VSNR A Wavelet-Based Visual Signal-to-Noise Ratio.pdf}
}

@article{CCT,
  title        = {Unified {{Blind Quality Assessment}} of {{Compressed Natural}}, {{Graphic}}, and {{Screen Content Images}}},
  author       = {Min, Xiongkuo and Ma, Kede and Gu, Ke and Zhai, Guangtao and Wang, Zhou and Lin, Weisi},
  date         = {2017-11},
  journaltitle = {IEEE Transactions on Image Processing},
  shortjournal = {IEEE Trans. on Image Process.},
  volume       = {26},
  number       = {11},
  pages        = {5462--5474},
  issn         = {1057-7149, 1941-0042},
  doi          = {10.1109/TIP.2017.2735192},
  url          = {http://ieeexplore.ieee.org/document/8000398/},
  urldate      = {2024-04-23},
  abstract     = {Digital images in the real world are created by a variety of means and have diverse properties. A photographical natural scene image (NSI) may exhibit substantially different characteristics from a computer graphic image (CGI) or a screen content image (SCI). This casts major challenges to objective image quality assessment, for which existing approaches lack effective mechanisms to capture such content type variations, and thus are difficult to generalize from one type to another. To tackle this problem, we first construct a cross-content-type (CCT) database, which contains 1,320 distorted NSIs, CGIs, and SCIs, compressed using the high efficiency video coding (HEVC) intra coding method and the screen content compression (SCC) extension of HEVC. We then carry out a subjective experiment on the database in a well-controlled laboratory environment. Moreover, we propose a unified content-type adaptive (UCA) blind image quality assessment model that is applicable across content types. A key step in UCA is to incorporate the variations of human perceptual characteristics in viewing different content types through a multi-scale weighting framework. This leads to superior performance on the constructed CCT database. UCA is training-free, implying strong generalizability. To verify this, we test UCA on other databases containing JPEG, MPEG-2, H.264, and HEVC compressed images/videos, and observe that it consistently achieves competitive performance.},
  langid       = {english},
  file         = {/Users/choekyelnyungmartsang/Zotero/storage/FCJP6NPM/Min et al. - 2017 - Unified Blind Quality Assessment of Compressed Nat.pdf}
}

@article{CID2013,
  title        = {{{CID2013}}: {{A Database}} for {{Evaluating No-Reference Image Quality Assessment Algorithms}}},
  shorttitle   = {{{CID2013}}},
  author       = {Virtanen, Toni and Nuutinen, Mikko and Vaahteranoksa, Mikko and Oittinen, Pirkko and Hakkinen, Jukka},
  date         = {2015-01},
  journaltitle = {IEEE Transactions on Image Processing},
  shortjournal = {IEEE Trans. on Image Process.},
  volume       = {24},
  number       = {1},
  pages        = {390--402},
  issn         = {1057-7149, 1941-0042},
  doi          = {10.1109/TIP.2014.2378061},
  url          = {http://ieeexplore.ieee.org/document/6975172/},
  urldate      = {2024-04-23},
  abstract     = {This paper presents a new database, CID2013, to address the issue of using no-reference (NR) image quality assessment algorithms on images with multiple distortions. Current NR algorithms struggle to handle images with many concurrent distortion types, such as real photographic images captured by different digital cameras. The database consists of six image sets; on average, 30 subjects have evaluated 12–14 devices depicting eight different scenes for a total of 79 different cameras, 480 images, and 188 subjects (67\% female). The subjective evaluation method was a hybrid absolute category rating-pair comparison developed for the study and presented in this paper. This method utilizes a slideshow of all images within a scene to allow the test images to work as references to each other. In addition to mean opinion score value, the images are also rated using sharpness, graininess, lightness, and color saturation scales. The CID2013 database contains images used in the experiments with the full subjective data plus extensive background information from the subjects. The database is made freely available for the research community.},
  langid       = {english},
  file         = {/Users/choekyelnyungmartsang/Zotero/storage/KHXSACRZ/Virtanen et al. - 2015 - CID2013 A Database for Evaluating No-Reference Im.pdf}
}

@article{CSIQ,
  title        = {Most Apparent Distortion: Full-Reference Image Quality Assessment and the Role of Strategy},
  shorttitle   = {Most Apparent Distortion},
  author       = {Chandler, Damon M.},
  date         = {2010-01-01},
  journaltitle = {Journal of Electronic Imaging},
  shortjournal = {J. Electron. Imaging},
  volume       = {19},
  number       = {1},
  pages        = {011006},
  issn         = {1017-9909},
  doi          = {10.1117/1.3267105},
  url          = {https://s2.smu.edu/~eclarson/pubs/2010JEI_MAD.pdf},
  urldate      = {2024-04-23},
  langid       = {english},
  file         = {/Users/choekyelnyungmartsang/Zotero/storage/ND7PFFNY/Chandler - 2010 - Most apparent distortion full-reference image qua.pdf}
}

@article{HSNID,
  title        = {Learning a {{Unified Blind Image Quality Metric}} via {{On-Line}} and {{Off-Line Big Training Instances}}},
  author       = {Gu, Ke and Xu, Xin and Qiao, Junfei and Jiang, Qiuping and Lin, Weisi and Thalmann, Daniel},
  date         = {2020-12-01},
  journaltitle = {IEEE Transactions on Big Data},
  shortjournal = {IEEE Trans. Big Data},
  volume       = {6},
  number       = {4},
  pages        = {780--791},
  issn         = {2332-7790, 2372-2096},
  doi          = {10.1109/TBDATA.2019.2895605},
  url          = {https://ieeexplore.ieee.org/document/8627983/},
  urldate      = {2024-04-23},
  abstract     = {In this work, we resolve a big challenge that most current image quality metrics (IQMs) are unavailable across different image contents, especially simultaneously coping with natural scene (NS) images or screen content (SC) images. By comparison with existing works, this paper deploys on-line and off-line data for proposing a unified no-reference (NR) IQM, not only applied to different distortion types and intensities but also to various image contents including classical NS images and prevailing SC images. Our proposed NR IQM is developed with two data-driven learning processes following feature extraction, which is based on scene statistic models, free-energy brain principle, and human visual system (HVS) characteristics. In the first process, the scene statistic models and an image retrieve technique are combined, based on on-line and off-line training instances, to derive a novel loose classifier for retrieving clean images and helping to infer the image content. In the second process, the features extracted by incorporating the inferred image content, free-energy and low-level perceptual characteristics of the HVS are learned by utilizing off-line training samples to analyze the distortion types and intensities and thereby to predict the image quality. The two processes mentioned above depend on a gigantic quantity of training data, much exceeding the number of images applied to performance validation, and thus make our model’s performance more reliable. Through extensive experiments, it has been validated that the proposed blind IQM is capable of simultaneously inferring the quality of NS and SC images, and it has attained superior performance as compared with popular and state-of-the-art IQMs on the subjective NS and SC image quality databases. The source code of our model will be released with the publication of the paper at https://kegu.netlify.com.},
  langid       = {english},
  file         = {/Users/choekyelnyungmartsang/Zotero/storage/8K778KAQ/Gu et al. - 2020 - Learning a Unified Blind Image Quality Metric via .pdf}
}

@inproceedings{KADID10k,
  title      = {{{KADID-10k}}: {{A Large-scale Artificially Distorted IQA Database}}},
  shorttitle = {{{KADID-10k}}},
  booktitle  = {2019 {{Eleventh International Conference}} on {{Quality}} of {{Multimedia Experience}} ({{QoMEX}})},
  author     = {Lin, Hanhe and Hosu, Vlad and Saupe, Dietmar},
  date       = {2019-06},
  pages      = {1--3},
  publisher  = {IEEE},
  location   = {Berlin, Germany},
  doi        = {10.1109/QoMEX.2019.8743252},
  url        = {https://ieeexplore.ieee.org/document/8743252/},
  urldate    = {2024-04-29},
  abstract   = {Current artificially distorted image quality assessment (IQA) databases are small in size and limited in content. Larger IQA databases that are diverse in content could benefit the development of deep learning for IQA. We create two datasets, the Konstanz Artificially Distorted Image quality Database (KADID-10k) and the Konstanz Artificially Distorted Image quality Set (KADIS-700k). The former contains 81 pristine images, each degraded by 25 distortions in 5 levels. The latter has 140,000 pristine images, with 5 degraded versions each, where the distortions are chosen randomly. We conduct a subjective IQA crowdsourcing study on KADID-10k to yield 30 degradation category ratings (DCRs) per image. We believe that the annotated set KADID-10k, together with the unlabelled set KADIS-700k, can enable the full potential of deep learning based IQA methods by means of weakly-supervised learning.},
  eventtitle = {2019 {{Eleventh International Conference}} on {{Quality}} of {{Multimedia Experience}} ({{QoMEX}})},
  isbn       = {978-1-5386-8212-8},
  langid     = {english},
  file       = {/Users/choekyelnyungmartsang/Zotero/storage/L26VZBXL/Lin et al. - 2019 - KADID-10k A Large-scale Artificially Distorted IQ.pdf}
}

@online{KADIS700k,
  title       = {{{DeepFL-IQA}}: {{Weak Supervision}} for {{Deep IQA Feature Learning}}},
  shorttitle  = {{{DeepFL-IQA}}},
  author      = {Lin, Hanhe and Hosu, Vlad and Saupe, Dietmar},
  date        = {2020-01-20},
  eprint      = {2001.08113},
  eprinttype  = {arxiv},
  eprintclass = {cs, eess},
  url         = {http://arxiv.org/abs/2001.08113},
  urldate     = {2024-04-29},
  abstract    = {Multi-level deep-features have been driving stateof-the-art methods for aesthetics and image quality assessment (IQA). However, most IQA benchmarks are comprised of artificially distorted images, for which features derived from ImageNet under-perform. We propose a new IQA dataset and a weakly supervised feature learning approach to train features more suitable for IQA of artificially distorted images. The dataset, KADIS-700k, is far more extensive than similar works, consisting of 140,000 pristine images, 25 distortions types, totaling 700k distorted versions. Our weakly supervised feature learning is designed as a multi-task learning type training, using eleven existing full-reference IQA metrics as proxies for differential mean opinion scores. We also introduce a benchmark database, KADID-10k, of artificially degraded images, each subjectively annotated by 30 crowd workers. We make use of our derived image feature vectors for (no-reference) image quality assessment by training and testing a shallow regression network on this database and five other benchmark IQA databases. Our method, termed DeepFL-IQA, performs better than other feature-based no-reference IQA methods and also better than all tested fullreference IQA methods on KADID-10k. For the other five benchmark IQA databases, DeepFL-IQA matches the performance of the best existing end-to-end deep learning-based methods on average.},
  langid      = {english},
  pubstate    = {preprint},
  keywords    = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file        = {/Users/choekyelnyungmartsang/Zotero/storage/QGCPLE9W/Lin et al. - 2020 - DeepFL-IQA Weak Supervision for Deep IQA Feature .pdf}
}

@article{LIVE,
  title        = {A {{Statistical Evaluation}} of {{Recent Full Reference Image Quality Assessment Algorithms}}},
  author       = {Sheikh, H.R. and Sabir, M.F. and Bovik, A.C.},
  date         = {2006-11},
  journaltitle = {IEEE Transactions on Image Processing},
  shortjournal = {IEEE Trans. on Image Process.},
  volume       = {15},
  number       = {11},
  pages        = {3440--3451},
  issn         = {1057-7149},
  doi          = {10.1109/TIP.2006.881959},
  url          = {http://ieeexplore.ieee.org/document/1709988/},
  urldate      = {2024-04-23},
  abstract     = {Measurement of visual quality is of fundamental importance for numerous image and video processing applications, where the goal of quality assessment (QA) algorithms is to automatically assess the quality of images or videos in agreement with human quality judgments. Over the years, many researchers have taken different approaches to the problem and have contributed significant research in this area and claim to have made progress in their respective domains. It is important to evaluate the performance of these algorithms in a comparative setting and analyze the strengths and weaknesses of these methods. In this paper, we present results of an extensive subjective quality assessment study in which a total of 779 distorted images were evaluated by about two dozen human subjects. The “ground truth” image quality data obtained from about 25 000 individual human quality judgments is used to evaluate the performance of several prominent full-reference image quality assessment algorithms. To the best of our knowledge, apart from video quality studies conducted by the Video Quality Experts Group, the study presented in this paper is the largest subjective image quality study in the literature in terms of number of images, distortion types, and number of human judgments per image. Moreover, we have made the data from the study freely available to the research community [1]. This would allow other researchers to easily report comparative results in the future.},
  langid       = {english},
  file         = {/Users/choekyelnyungmartsang/Zotero/storage/8F2JWIAS/Sheikh et al. - 2006 - A Statistical Evaluation of Recent Full Reference .pdf}
}

@article{LIVE_Wild,
  title        = {Massive {{Online Crowdsourced Study}} of {{Subjective}} and {{Objective Picture Quality}}},
  author       = {Ghadiyaram, Deepti and Bovik, Alan C.},
  date         = {2016-01},
  journaltitle = {IEEE Transactions on Image Processing},
  shortjournal = {IEEE Trans. on Image Process.},
  volume       = {25},
  number       = {1},
  pages        = {372--387},
  issn         = {1057-7149, 1941-0042},
  doi          = {10.1109/TIP.2015.2500021},
  url          = {http://ieeexplore.ieee.org/document/7327186/},
  urldate      = {2024-04-23},
  abstract     = {Most publicly available image quality databases have been created under highly controlled conditions by introducing graded simulated distortions onto high-quality photographs. However, images captured using typical real-world mobile camera devices are usually afflicted by complex mixtures of multiple distortions, which are not necessarily well-modeled by the synthetic distortions found in existing databases. The originators of existing legacy databases usually conducted human psychometric studies to obtain statistically meaningful sets of human opinion scores on images in a stringently controlled visual environment, resulting in small data collections relative to other kinds of image analysis databases. Toward overcoming these limitations, we designed and created a new database that we call the LIVE In the Wild Image Quality Challenge Database, which contains widely diverse authentic image distortions on a large number of images captured using a representative variety of modern mobile devices. We also designed and implemented a new online crowdsourcing system, which we have used to conduct a very large-scale, multi-month image quality assessment (IQA) subjective study. Our database consists of over 350 000 opinion scores on 1162 images evaluated by over 8100 unique human observers. Despite the lack of control over the experimental environments of the numerous study participants, we demonstrate excellent internal consistency of the subjective data set. We also evaluate several top-performing blind IQA algorithms on it and present insights on how the mixtures of distortions challenge both end users as well as automatic perceptual quality prediction models. The new database is available for public use at http://live.ece.utexas.edu/research/ChallengeDB/index.html.},
  langid       = {english},
  file         = {/Users/choekyelnyungmartsang/Zotero/storage/NQWGF6QQ/Ghadiyaram und Bovik - 2016 - Massive Online Crowdsourced Study of Subjective an.pdf}
}

@inproceedings{LIVEMD,
  title      = {Objective Quality Assessment of Multiply Distorted Images},
  booktitle  = {2012 {{Conference Record}} of the {{Forty Sixth Asilomar Conference}} on {{Signals}}, {{Systems}} and {{Computers}} ({{ASILOMAR}})},
  author     = {Jayaraman, Dinesh and Mittal, Anish and Moorthy, Anush K. and Bovik, Alan C.},
  date       = {2012-11},
  pages      = {1693--1697},
  publisher  = {IEEE},
  location   = {Pacific Grove, CA, USA},
  doi        = {10.1109/ACSSC.2012.6489321},
  url        = {http://ieeexplore.ieee.org/document/6489321/},
  urldate    = {2024-04-23},
  abstract   = {Subjective studies have been conducted in the past to obtain human judgments of visual quality on distorted images in order, among other things, to benchmark objective image qual. ity assessment (lQA) algorithms. Existing subjective studies primarily have records of human ratings on images that were corrupted by only one of many possible distortions. However, the majority of images that are available for consumption are corrupted by multiple distortions. Towards broadening the corpora of records of human responses to visual distortions, we recently conducted a study on two types of multiply distorted images to obtain human judgments of the visual quality of such images. Further, we compared the performance of several existing objective image quality measures on the new database and analyze the effects of multiple distortions on commonly used quality-determinant features and on human ratings.},
  eventtitle = {2012 46th {{Asilomar Conference}} on {{Signals}}, {{Systems}} and {{Computers}}},
  langid     = {english},
  file       = {/Users/choekyelnyungmartsang/Zotero/storage/XIL22H3U/Jayaraman et al. - 2012 - Objective quality assessment of multiply distorted.pdf}
}

@article{MDID2013,
  title        = {Hybrid {{No-Reference Quality Metric}} for {{Singly}} and {{Multiply Distorted Images}}},
  author       = {Gu, Ke and Zhai, Guangtao and Yang, Xiaokang and Zhang, Wenjun},
  date         = {2014-09},
  journaltitle = {IEEE Transactions on Broadcasting},
  shortjournal = {IEEE Trans. on Broadcast.},
  volume       = {60},
  number       = {3},
  pages        = {555--567},
  issn         = {0018-9316, 1557-9611},
  doi          = {10.1109/TBC.2014.2344471},
  url          = {https://ieeexplore.ieee.org/document/6879255/},
  urldate      = {2024-04-23},
  abstract     = {In a typical image communication system, the visual signal presented to the end users may undergo the steps of acquisition, compression and transmission which cause the artifacts of blurring, quantization and noise. However, the researches of image quality assessment (IQA) with multiple distortion types are very limited. In this paper, we first introduce a new multiply distorted image database (MDID2013), which is composed of 324 images that are simultaneously corrupted by blurring, JPEG compression and noise injection. We then propose a new six-step blind metric (SISBLIM) for quality assessment of both singly and multiply distorted images. Inspired by the early human visual model and recently revealed free energy based brain theory, our method works to systematically combine the single quality prediction of each emerging distortion type and joint effects of different distortion sources. Comparative studies of the proposed SISBLIM with popular full-reference IQA approaches and start-of-the-art no-reference IQA metrics are conducted on five singly distorted image databases (LIVE, TID2008, CSIQ, IVC, Toyama) and two newly released multiply distorted image databases (LIVEMD, MDID2013). Experimental results confirm the effectiveness of our blind technique. MATLAB codes of the proposed SISBLIM algorithm and MDID2013 database will be available online at http://gvsp.sjtu.edu.cn/.},
  langid       = {english},
  file         = {/Users/choekyelnyungmartsang/Zotero/storage/B4SN597A/Gu et al. - 2014 - Hybrid No-Reference Quality Metric for Singly and .pdf}
}

@article{MDID2016,
  title        = {{{MDID}}: {{A}} Multiply Distorted Image Database for Image Quality Assessment},
  shorttitle   = {{{MDID}}},
  author       = {Sun, Wen and Zhou, Fei and Liao, Qingmin},
  date         = {2017-01},
  journaltitle = {Pattern Recognition},
  shortjournal = {Pattern Recognition},
  volume       = {61},
  pages        = {153--168},
  issn         = {00313203},
  doi          = {10.1016/j.patcog.2016.07.033},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S0031320316301911},
  urldate      = {2024-04-23},
  abstract     = {In this paper, we present a new database, the multiply distorted image database (MDID), to evaluate image quality assessment (IQA) metrics on multiply distorted images. The database contains 20 reference images and 1600 distorted images. The latter images are obtained by contamination of the former with multiple distortions of random types and levels, so multiple types of distortions appear in each distorted image. Pair comparison sorting (PCS) is used as a new subjective rating method to evaluate image quality. This method allows subjects to make equal decisions on images whose difference in quality cannot be easily evaluated visually. A total of 192 subjects participated in the subjective rating, in which mean opinion scores and standard deviations were obtained. In IQA research, subjective scores and algorithm predictions are generally related by a nonlinear regression. We further propose a method to initialize the parameters of the nonlinear regression. The experiments of IQA metrics conducted on MDID validate that this database is advisable and challenging.},
  langid       = {english},
  file         = {/Users/choekyelnyungmartsang/Zotero/storage/JY7KJWKT/Sun et al. - 2017 - MDID A multiply distorted image database for imag.pdf}
}

@article{SaF,
  title        = {Store-and-{{Forward Images}} in {{Teledermatology}}: {{Narrative Literature Review}}},
  shorttitle   = {Store-and-{{Forward Images}} in {{Teledermatology}}},
  author       = {Jiang, Simon W and Flynn, Michael Seth and Kwock, Jeffery T and Nicholas, Matilda W},
  date         = {2022-07-18},
  journaltitle = {JMIR Dermatology},
  shortjournal = {JMIR Dermatol},
  volume       = {5},
  number       = {3},
  pages        = {e37517},
  issn         = {2562-0959},
  doi          = {10.2196/37517},
  url          = {https://derma.jmir.org/2022/3/e37517},
  urldate      = {2024-05-21},
  abstract     = {Background: Store-and-forward (SAF) teledermatology uses electronically stored information, including patient photographs and demographic information, for clinical decision-making asynchronous to the patient encounter. The integration of SAF teledermatology into clinical practice has been increasing in recent years, especially during the COVID-19 pandemic. Despite this growth, data regarding the outcomes of SAF teledermatology are limited. A key distinction among current literature involves comparing the quality and utility of images obtained by patients and trained clinicians, as these metrics may vary by the clinical expertise of the photographer. Objective: This narrative literature review aimed to characterize the outcomes of SAF teledermatology through the lens of patient- versus clinician-initiated photography and highlight important future directions for and challenges of the field. Methods: A literature search of peer-reviewed research was performed between February and April 2021. Key search terms included patient-initiated, patient-submitted, clinician-initiated, clinician-submitted, store-and-forward, asynchronous, remote, image, photograph, and teledermatology. Only studies published after 2001 in English were included. In total, 47 studies were identified from the PubMed electronic database and Google Scholar after omitting duplicate articles. Results: Image quality and diagnostic concordance are generally lower and more variable with patient-submitted images, which may impact their decision-making utility. SAF teledermatology can improve the efficiency of and access to care when photographs are taken by either clinicians or patients. The clinical outcomes of clinician-submitted images are comparable to those of in-person visits in the few studies that have investigated these outcomes. Coinciding with the onset of the COVID-19 pandemic, asynchronous teledermatology helped minimize unnecessary in-person visits in the outpatient setting, as many uncomplicated conditions could be adequately managed remotely via images captured by patients and referring clinicians. For the inpatient setting, SAF teledermatology minimized unnecessary contact during dermatology consultations, although current studies are limited by the heterogeneity of their outcomes. Conclusions: In general, photographs taken by trained clinicians are higher quality and have better and more relevant diagnostic and clinical outcomes. SAF teledermatology helped clinicians avoid unnecessary physical contact with patients in the outpatient and inpatient settings during the COVID-19 pandemic. Asynchronous teledermatology will likely play a greater role in the future as SAF images become integrated into synchronous teledermatology workflows. However, the obstacles summarized in this review should be addressed before its widespread implementation into clinical practice.},
  langid       = {english},
  file         = {/Users/choekyelnyungmartsang/Zotero/storage/IRHBXM7R/Jiang et al. - 2022 - Store-and-Forward Images in Teledermatology Narra.pdf}
}

@article{SCIQ,
  title        = {{{ESIM}}: {{Edge Similarity}} for {{Screen Content Image Quality Assessment}}},
  shorttitle   = {{{ESIM}}},
  author       = {Ni, Zhangkai and Ma, Lin and Zeng, Huanqiang and Chen, Jing and Cai, Canhui and Ma, Kai-Kuang},
  date         = {2017-10},
  journaltitle = {IEEE Transactions on Image Processing},
  shortjournal = {IEEE Trans. on Image Process.},
  volume       = {26},
  number       = {10},
  pages        = {4818--4831},
  issn         = {1057-7149, 1941-0042},
  doi          = {10.1109/TIP.2017.2718185},
  url          = {http://ieeexplore.ieee.org/document/7954714/},
  urldate      = {2024-04-23},
  abstract     = {In this paper, an accurate full-reference image quality assessment (IQA) model developed for assessing screen content images (SCIs), called the edge similarity (ESIM), is proposed. It is inspired by the fact that the human visual system (HVS) is highly sensitive to edges that are often encountered in SCIs; therefore, essential edge features are extracted and exploited for conducting IQA for the SCIs. The key novelty of the proposed ESIM lies in the extraction and use of three salient edge features—i.e., edge contrast, edge width, and edge direction. The first two attributes are simultaneously generated from the input SCI based on a parametric edge model, while the last one is derived directly from the input SCI. The extraction of these three features will be performed for the reference SCI and the distorted SCI, individually. The degree of similarity measured for each above-mentioned edge attribute is then computed independently, followed by combining them together using our proposed edge-width pooling strategy to generate the final ESIM score. To conduct the performance evaluation of our proposed ESIM model, a new and the largest SCI database (denoted as SCID) is established in our work and made to the public for download. Our database contains 1800 distorted SCIs that are generated from 40 reference SCIs. For each SCI, nine distortion types are investigated, and five degradation levels are produced for each distortion type. Extensive simulation results have clearly shown that the proposed ESIM model is more consistent with the perception of the HVS on the evaluation of distorted SCIs than the multiple state-of-the-art IQA methods.},
  langid       = {english},
  file         = {/Users/choekyelnyungmartsang/Zotero/storage/KUNRFCHU/Ni et al. - 2017 - ESIM Edge Similarity for Screen Content Image Qua.pdf}
}

@inproceedings{SIQAD,
  title      = {Subjective Quality Assessment of {{Screen Content Images}}},
  booktitle  = {2014 {{Sixth International Workshop}} on {{Quality}} of {{Multimedia Experience}} ({{QoMEX}})},
  author     = {Yang, Huan and {Yuming Fang} and Lin, Weisi and Wang, Zhou},
  date       = {2014-09},
  pages      = {257--262},
  publisher  = {IEEE},
  location   = {Singapore, Singapore},
  doi        = {10.1109/QoMEX.2014.6982328},
  url        = {http://ieeexplore.ieee.org/document/6982328/},
  urldate    = {2024-04-23},
  abstract   = {Research on Screen Content Images (SCIs) becomes important as they are increasingly used in multi-device communication applications. In this paper, we present a study of subjective quality assessment for distorted SCIs, and investigate which part (text or picture) contributes more to the overall visual quality. We construct a large-scale Screen Image Quality Assessment Database (SIQAD) consisting of 20 source and 980 distorted SCIs. The 11-category Absolute Category Rating (ACR) is employed to obtain three subjective quality scores corresponding to the entire image, textual and pictorial regions respectively. Based on the subjective data, we investigate the applicability of 12 state-of-the-art Image Quality Assessment (IQA) methods for objectively assessing the quality of SCIs. The results indicate that existing IQA methods are limited in predicting human quality judgement of SCIs. Moreover, we propose a prediction model to account for the correlation between the subjective scores of textual and pictorial regions and the entire image. The current results make an initial move towards objective quality assessment of SCIs.},
  eventtitle = {2014 {{Sixth International Workshop}} on {{Quality}} of {{Multimedia Experience}} ({{QoMEX}})},
  isbn       = {978-1-4799-6536-6},
  langid     = {english},
  file       = {/Users/choekyelnyungmartsang/Zotero/storage/46AJMFUP/Yang et al. - 2014 - Subjective quality assessment of Screen Content Im.pdf}
}


@article{TID2008,
  title    = {{{TID2008}} – {{A Database}} for {{Evaluation}} of {{Full- Reference Visual Quality Assessment Metrics}}},
  author   = {Ponomarenko, Nikolay and Lukin, Vladimir and Zelensky, Alexander and Egiazarian, Karen and Astola, Jaakko and Carli, Marco and Battisti, Federica},
  date     = {2009-01},
  abstract = {In this paper, a new image database, TID2008, for evaluation of full-reference visual quality assessment metrics is described. It contains 1700 test images (25 reference images, 17 types of distortions for each reference image, 4 different levels of each type of distortion). Mean Opinion Scores (MOS) for this database have been obtained as a result of more than 800 experiments. During these tests, observers from three countries (Finland, Italy, and Ukraine) have carried out about 256000 individual human quality judgments. The obtained MOS can be used for effective testing of different visual quality metrics as well as for the design of new metrics. Using the designed image database, we have tested several known quality metrics. The designed test image database is freely available for downloading and utilization in scientific investigations.},
  langid   = {english},
  file     = {/Users/choekyelnyungmartsang/Zotero/storage/M93CPMPI/Ponomarenko et al. - TID2008 – A Database for Evaluation of Full- Refer.pdf}
}

@article{TID2013,
  title        = {Image Database {{TID2013}}: {{Peculiarities}}, Results and Perspectives},
  shorttitle   = {Image Database {{TID2013}}},
  author       = {Ponomarenko, Nikolay and Jin, Lina and Ieremeiev, Oleg and Lukin, Vladimir and Egiazarian, Karen and Astola, Jaakko and Vozel, Benoit and Chehdi, Kacem and Carli, Marco and Battisti, Federica and Jay Kuo, C.-C.},
  date         = {2015-01},
  journaltitle = {Signal Processing: Image Communication},
  shortjournal = {Signal Processing: Image Communication},
  volume       = {30},
  pages        = {57--77},
  issn         = {09235965},
  doi          = {10.1016/j.image.2014.10.009},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S0923596514001490},
  urldate      = {2024-04-23},
  abstract     = {This paper describes a recently created image database, TID2013, intended for evaluation of full-reference visual quality assessment metrics. With respect to TID2008, the new database contains a larger number (3000) of test images obtained from 25 reference images, 24 types of distortions for each reference image, and 5 levels for each type of distortion. Motivations for introducing 7 new types of distortions and one additional level of distortions are given; examples of distorted images are presented. Mean opinion scores (MOS) for the new database have been collected by performing 985 subjective experiments with volunteers (observers) from five countries (Finland, France, Italy, Ukraine, and USA). The availability of MOS allows the use of the designed database as a fundamental tool for assessing the effectiveness of visual quality. Furthermore, existing visual quality metrics have been tested with the proposed database and the collected results have been analyzed using rank order correlation coefficients between MOS and considered metrics. These correlation indices have been obtained both considering the full set of distorted images and specific image subsets, for highlighting advantages and drawbacks of existing, state of the art, quality metrics. Approaches to thorough performance analysis for a given metric are presented to detect practical situations or distortion types for which this metric is not adequate enough to human perception. The created image database and the collected MOS values are freely available for downloading and utilization for scientific purposes.},
  langid       = {english},
  file         = {/Users/choekyelnyungmartsang/Zotero/storage/NSWLLPSB/Ponomarenko et al. - 2015 - Image database TID2013 Peculiarities, results and.pdf}
}

@article{TDCriteria,
  title = {Proposed {{Technical Guidelines}} for the {{Acquisition}} of {{Clinical Images}} of {{Skin-Related Conditions}}},
  author = {Finnane, Anna and Curiel-Lewandrowski, Clara and Wimberley, Glen and Caffery, Liam and Katragadda, Chinmayee and Halpern, Allan and Marghoob, Ashfaq A. and Malvehy, Josep and Kittler, Harald and Hofmann-Wellenhof, Rainer and Abraham, Ivo and Soyer, H. Peter and {On behalf of the International Society of Digital Imaging of the Skin (ISDIS) for the International Skin Imaging Collaboration (ISIC)}},
  date = {2017-05-01},
  journaltitle = {JAMA Dermatology},
  shortjournal = {JAMA Dermatol},
  volume = {153},
  number = {5},
  pages = {453},
  issn = {2168-6068},
  doi = {10.1001/jamadermatol.2016.6214},
  url = {http://archderm.jamanetwork.com/article.aspx?doi=10.1001/jamadermatol.2016.6214},
  urldate = {2024-04-23},
  abstract = {OBJECTIVE To provide guidelines for the clinical application of the Standards for Dermatological Imaging set forward by the ISIC. EVIDENCE REVIEW The ISIC recommendations were developed through a hybrid Delphi methodology. The methods for achieving consensus have been described previously. The practical application of these recommendations was evaluated by 2 clinical photographers with expertise in skin imaging. Images corresponding to each recommendation were taken by a clinical photographer and provided as visual examples of how these recommendations can be implemented in clinical practice. RESULTS The Standards for Dermatological Imaging developed by the ISIC members could be followed in the clinical setting. Images showing appropriate lighting, background color, field of view, image orientation, focus and depth of field, resolution, and scale and color calibration were obtained by the clinical photographer, by following the detailed recommendations for regional, close-up and dermoscopic images. CONCLUSIONS AND RELEVANCE Adhering to the recommendations is both feasible and achievable in practice. Adopting these Standards is the first step in achieving international standardization of skin imaging, with the potential to improve clinical outcomes and research activities.},
  langid = {english},
  file = {/Users/choekyelnyungmartsang/Zotero/storage/HWNCWHI6/Finnane et al. - 2017 - Proposed Technical Guidelines for the Acquisition .pdf}
}

@article{WED,
  title        = {Waterloo {{Exploration Database}}: {{New Challenges}} for {{Image Quality Assessment Models}}},
  shorttitle   = {Waterloo {{Exploration Database}}},
  author       = {Ma, Kede and Duanmu, Zhengfang and Wu, Qingbo and Wang, Zhou and Yong, Hongwei and Li, Hongliang and Zhang, Lei},
  date         = {2017-02},
  journaltitle = {IEEE Transactions on Image Processing},
  shortjournal = {IEEE Trans. on Image Process.},
  volume       = {26},
  number       = {2},
  pages        = {1004--1016},
  issn         = {1057-7149, 1941-0042},
  doi          = {10.1109/TIP.2016.2631888},
  url          = {http://ieeexplore.ieee.org/document/7752930/},
  urldate      = {2024-04-23},
  abstract     = {The great content diversity of real-world digital images poses a grand challenge to image quality assessment (IQA) models, which are traditionally designed and validated on a handful of commonly used IQA databases with very limited content variation. To test the generalization capability and to facilitate the wide usage of IQA techniques in real-world applications, we establish a large-scale database named the Waterloo Exploration Database, which in its current state contains 4744 pristine natural images and 94 880 distorted images created from them. Instead of collecting the mean opinion score for each image via subjective testing, which is extremely difficult if not impossible, we present three alternative test criteria to evaluate the performance of IQA models, namely, the pristine/distorted image discriminability test, the listwise ranking consistency test, and the pairwise preference consistency test (P-test). We compare 20 well-known IQA models using the proposed criteria, which not only provide a stronger test in a more challenging testing environment for existing models, but also demonstrate the additional benefits of using the proposed database. For example, in the P-test, even for the best performing no-reference IQA model, more than 6 million failure cases against the model are “discovered” automatically out of over 1 billion test pairs. Furthermore, we discuss how the new database may be exploited using innovative approaches in the future, to reveal the weaknesses of existing IQA models, to provide insights on how to improve the models, and to shed light on how the next-generation IQA models may be developed. The database and codes are made publicly available at: https://ece.uwaterloo.ca/\textasciitilde k29ma/exploration/.},
  langid       = {english},
  file         = {/Users/choekyelnyungmartsang/Zotero/storage/CZGVWNWD/Ma et al. - 2017 - Waterloo Exploration Database New Challenges for .pdf}
}

@inproceedings{ACNE04,
  title      = {Joint {{Acne Image Grading}} and {{Counting}} via {{Label Distribution Learning}}},
  booktitle  = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author     = {Wu, Xiaoping and Wen, Ni and Liang, Jie and Lai, Yu-Kun and She, Dongyu and Cheng, Ming-Ming and Yang, Jufeng},
  date       = {2019-10},
  pages      = {10641--10650},
  publisher  = {IEEE},
  location   = {Seoul, Korea (South)},
  doi        = {10.1109/ICCV.2019.01074},
  url        = {https://ieeexplore.ieee.org/document/9010021/},
  urldate    = {2024-04-28},
  abstract   = {Accurate grading of skin disease severity plays a crucial role in precise treatment for patients. Acne vulgaris, the most common skin disease in adolescence, can be graded by evidence-based lesion counting as well as experiencebased global estimation in the medical field. However, due to the appearance similarity of acne with close severity, it is challenging to count and grade acne accurately. In this paper, we address the problem of acne image analysis via Label Distribution Learning (LDL) considering the ambiguous information among acne severity. Based on the professional grading criterion, we generate two acne label distributions considering the relationship between the similar number of lesions and severity of acne, respectively. We also propose a unified framework for joint acne image grading and counting, which is optimized by the multi-task learning loss. In addition, we further build the ACNE04 dataset with annotations of acne severity and lesion number of each image for evaluation. Experiments demonstrate that our proposed framework performs favorably against stateof-the-art methods. We make the code and dataset publicly available at https://github.com/xpwu95/ldl.},
  eventtitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn       = {978-1-72814-803-8},
  langid     = {english},
  file       = {/Users/choekyelnyungmartsang/Zotero/storage/43KJG5LJ/Wu et al. - 2019 - Joint Acne Image Grading and Counting via Label Di.pdf}
}

@article{DDI,
  title        = {Disparities in Dermatology {{AI}} Performance on a Diverse, Curated Clinical Image Set},
  author       = {Daneshjou, Roxana and Vodrahalli, Kailas and Novoa, Roberto A. and Jenkins, Melissa and Liang, Weixin and Rotemberg, Veronica and Ko, Justin and Swetter, Susan M. and Bailey, Elizabeth E. and Gevaert, Olivier and Mukherjee, Pritam and Phung, Michelle and Yekrang, Kiana and Fong, Bradley and Sahasrabudhe, Rachna and Allerup, Johan A. C. and Okata-Karigane, Utako and Zou, James and Chiou, Albert S.},
  date         = {2022-08-12},
  journaltitle = {Science Advances},
  shortjournal = {Sci. Adv.},
  volume       = {8},
  number       = {32},
  pages        = {eabq6147},
  issn         = {2375-2548},
  doi          = {10.1126/sciadv.abq6147},
  url          = {https://www.science.org/doi/10.1126/sciadv.abq6147},
  urldate      = {2024-04-28},
  abstract     = {An estimated 3 billion people lack access to dermatological care globally. Artificial intelligence (AI) may aid in triaging skin diseases and identifying malignancies. However, most AI models have not been assessed on images of diverse skin tones or uncommon diseases. Thus, we created the Diverse Dermatology Images (DDI) dataset—the first publicly available, expertly curated, and pathologically confirmed image dataset with diverse skin tones. We show that state-of-the-art dermatology AI models exhibit substantial limitations on the DDI dataset, particularly on dark skin tones and uncommon diseases. We find that dermatologists, who often label AI datasets, also perform worse on images of dark skin tones and uncommon diseases. Fine-tuning AI models on the DDI images closes the performance gap between light and dark skin tones. These findings identify important weaknesses and biases in dermatology AI that should be addressed for reliable application to diverse patients and diseases.           ,              A diverse, curated clinical dermatology image set can help address AI in dermatology limitations.},
  langid       = {english},
  file         = {/Users/choekyelnyungmartsang/Zotero/storage/ECM8UI47/Daneshjou et al. - 2022 - Disparities in dermatology AI performance on a div.pdf}
}

@article{Derm7pt,
  title        = {Seven-{{Point Checklist}} and {{Skin Lesion Classification Using Multitask Multimodal Neural Nets}}},
  author       = {Kawahara, Jeremy and Daneshvar, Sara and Argenziano, Giuseppe and Hamarneh, Ghassan},
  date         = {2019-03},
  journaltitle = {IEEE Journal of Biomedical and Health Informatics},
  shortjournal = {IEEE J. Biomed. Health Inform.},
  volume       = {23},
  number       = {2},
  pages        = {538--546},
  issn         = {2168-2194, 2168-2208},
  doi          = {10.1109/JBHI.2018.2824327},
  url          = {https://ieeexplore.ieee.org/document/8333693/},
  urldate      = {2024-04-28},
  abstract     = {We propose a multitask deep convolutional neural network, trained on multimodal data (clinical and dermoscopic images, and patient metadata), to classify the 7-point melanoma checklist criteria and perform skin lesion diagnosis. Our neural network is trained using several multitask loss functions, where each loss considers different combinations of the input modalities, which allows our model to be robust to missing data at inference time. Our final model classifies the 7-point checklist and skin condition diagnosis, produces multimodal feature vectors suitable for image retrieval, and localizes clinically discriminant regions. We benchmark our approach using 1011 lesion cases, and report comprehensive results over all 7-point criteria and diagnosis. We also make our dataset (images and metadata) publicly available online at http://derm.cs.sfu.ca.},
  langid       = {english},
  file         = {/Users/choekyelnyungmartsang/Zotero/storage/X3NDWJDI/Kawahara et al. - 2019 - Seven-Point Checklist and Skin Lesion Classificati.pdf}
}

@online{F17K,
  title       = {Evaluating {{Deep Neural Networks Trained}} on {{Clinical Images}} in {{Dermatology}} with the {{Fitzpatrick}} 17k {{Dataset}}},
  author      = {Groh, Matthew and Harris, Caleb and Soenksen, Luis and Lau, Felix and Han, Rachel and Kim, Aerin and Koochek, Arash and Badri, Omar},
  date        = {2021-04-20},
  eprint      = {2104.09957},
  eprinttype  = {arxiv},
  eprintclass = {cs},
  url         = {http://arxiv.org/abs/2104.09957},
  urldate     = {2024-04-28},
  abstract    = {How does the accuracy of deep neural network models trained to classify clinical images of skin conditions vary across skin color? While recent studies demonstrate computer vision models can serve as a useful decision support tool in healthcare and provide dermatologist-level classification on a number of specific tasks, darker skin is underrepresented in the data. Most publicly available data sets do not include Fitzpatrick skin type labels. We annotate 16,577 clinical images sourced from two dermatology atlases with Fitzpatrick skin type labels and open-source these annotations. Based on these labels, we find that there are significantly more images of light skin types than dark skin types in this dataset. We train a deep neural network model to classify 114 skin conditions and find that the model is most accurate on skin types similar to those it was trained on. In addition, we evaluate how an algorithmic approach to identifying skin tones, individual typology angle, compares with Fitzpatrick skin type labels annotated by a team of human labelers.},
  langid      = {english},
  pubstate    = {preprint},
  keywords    = {Computer Science - Computer Vision and Pattern Recognition},
  file        = {/Users/choekyelnyungmartsang/Zotero/storage/7CFK8V66/Groh et al. - 2021 - Evaluating Deep Neural Networks Trained on Clinica.pdf}
}

@online{Monkeypox,
  title       = {Monkeypox {{Image Data}} Collection},
  author      = {Ahsan, Md Manjurul and Uddin, Muhammad Ramiz and Luna, Shahana Akter},
  date        = {2022-06-03},
  eprint      = {2206.01774},
  eprinttype  = {arxiv},
  eprintclass = {cs, eess},
  url         = {http://arxiv.org/abs/2206.01774},
  urldate     = {2024-04-28},
  abstract    = {This paper explains the initial Monkeypox Open image data collection procedure. It was created by assembling images collected from websites, newspapers, and online portals and currently contains around 1905 images after data augmentation.},
  langid      = {english},
  pubstate    = {preprint},
  keywords    = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file        = {/Users/choekyelnyungmartsang/Zotero/storage/J3MR87R2/Ahsan et al. - 2022 - Monkeypox Image Data collection.pdf}
}

@article{PAD-UFES-20,
  title        = {The Impact of Patient Clinical Information on Automated Skin Cancer Detection},
  author       = {Pacheco, Andre G.C. and Krohling, Renato A.},
  date         = {2020-01},
  journaltitle = {Computers in Biology and Medicine},
  shortjournal = {Computers in Biology and Medicine},
  volume       = {116},
  pages        = {103545},
  issn         = {00104825},
  doi          = {10.1016/j.compbiomed.2019.103545},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S0010482519304019},
  urldate      = {2024-04-28},
  abstract     = {Skin cancer is one of the most common types of cancer worldwide. Over the past few years, different approaches have been proposed to deal with automated skin cancer detection. Nonetheless, most of them are based only on dermoscopic images and do not take into account the patient clinical information, an important clue towards clinical diagnosis. In this work, we present an approach to fill this gap. First, we introduce a new dataset composed of clinical images, collected using smartphones, and clinical data related to the patient. Next, we propose a straightforward method that includes an aggregation mechanism in well-known deep learning models to combine features from images and clinical data. Last, we carry out experiments to compare the models’ performance with and without using this mechanism. The results present an improvement of approximately 7\% in balanced accuracy when the aggregation method is applied. Overall, the impact of clinical data on models’ performance is significant and shows the importance of including these features on automated skin cancer detection.},
  langid       = {english},
  file         = {/Users/choekyelnyungmartsang/Zotero/storage/8GRBMVLI/Pacheco und Krohling - 2020 - The impact of patient clinical information on auto.pdf}
}

@online{SCIN,
  title       = {Crowdsourcing {{Dermatology Images}} with {{Google Search Ads}}: {{Creating}} a {{Real-World Skin Condition Dataset}}},
  shorttitle  = {Crowdsourcing {{Dermatology Images}} with {{Google Search Ads}}},
  author      = {Ward, Abbi and Li, Jimmy and Wang, Julie and Lakshminarasimhan, Sriram and Carrick, Ashley and Campana, Bilson and Hartford, Jay and S, Pradeep Kumar and Tiyasirichokchai, Tiya and Virmani, Sunny and Wong, Renee and Matias, Yossi and Corrado, Greg S. and Webster, Dale R. and Siegel, Dawn and Lin, Steven and Ko, Justin and Karthikesalingam, Alan and Semturs, Christopher and Rao, Pooja},
  date        = {2024-02-28},
  eprint      = {2402.18545},
  eprinttype  = {arxiv},
  eprintclass = {cs},
  url         = {http://arxiv.org/abs/2402.18545},
  urldate     = {2024-04-28},
  abstract    = {Background: Health datasets from clinical sources do not reflect the breadth and diversity of disease in the real world, impacting research, medical education, and artificial intelligence (AI) tool development. Dermatology is a suitable area to develop and test a new and scalable method to create representative health datasets. Methods: We used Google Search advertisements to invite contributions to an open access dataset of images of dermatology conditions, demographic, and symptom information. With informed contributor consent, we describe and release this dataset containing 10,408 images from 5,033 contributions from internet users in the United States over 8 months starting March 2023. The dataset includes dermatologist condition labels as well as estimated Fitzpatrick Skin Type (eFST) and Monk Skin Tone (eMST) labels for the images. Results: We received a median of 22 submissions/day (IQR 14–30). Female (66.72\%) and younger (52\% {$<$} age 40) contributors had a higher representation in the dataset compared to the US population, and 32.6\% of contributors reported a non-White racial or ethnic identity. Over 97.5\% of contributions were genuine images of skin conditions. Dermatologist confidence in assigning a differential diagnosis increased with the number of available variables, and showed a weak correlation with image sharpness (Spearman’s P values {$<$} 0.001 and 0.01 respectively). Most contributions were short-duration (54\% with onset {$<$} 7 days ago ) and 89\% were allergic, infectious, or inflammatory conditions. eFST and eMST distributions reflected the geographical origin of the dataset. The dataset is available at github.com/google-researchdatasets/scin. Conclusion: Search ads are effective at crowdsourcing images of health conditions. The SCIN dataset bridges important gaps in the availability of representative images of common skin conditions.},
  langid      = {english},
  pubstate    = {preprint},
  keywords    = {Computer Science - Computers and Society},
  file        = {/Users/choekyelnyungmartsang/Zotero/storage/UQGXAEAY/Ward et al. - 2024 - Crowdsourcing Dermatology Images with Google Searc.pdf}
}

@online{ARNIQA,
  title       = {{{ARNIQA}}: {{Learning Distortion Manifold}} for {{Image Quality Assessment}}},
  shorttitle  = {{{ARNIQA}}},
  author      = {Agnolucci, Lorenzo and Galteri, Leonardo and Bertini, Marco and Del Bimbo, Alberto},
  date        = {2023-11-04},
  eprint      = {2310.14918},
  eprinttype  = {arxiv},
  eprintclass = {cs},
  url         = {http://arxiv.org/abs/2310.14918},
  urldate     = {2024-04-23},
  abstract    = {No-Reference Image Quality Assessment (NR-IQA) aims to develop methods to measure image quality in alignment with human perception without the need for a high-quality reference image. In this work, we propose a self-supervised approach named ARNIQA (leArning distoRtion maNifold for Image Quality Assessment) for modeling the image distortion manifold to obtain quality representations in an intrinsic manner. First, we introduce an image degradation model that randomly composes ordered sequences of consecutively applied distortions. In this way, we can synthetically degrade images with a large variety of degradation patterns. Second, we propose to train our model by maximizing the similarity between the representations of patches of different images distorted equally, despite varying content. Therefore, images degraded in the same manner correspond to neighboring positions within the distortion manifold. Finally, we map the image representations to the quality scores with a simple linear regressor, thus without fine-tuning the encoder weights. The experiments show that our approach achieves state-of-the-art performance on several datasets. In addition, ARNIQA demonstrates improved data efficiency, generalization capabilities, and robustness compared to competing methods. The code and the model are publicly available at https://github.com/ miccunifi/ARNIQA.},
  langid      = {english},
  pubstate    = {preprint},
  keywords    = {Computer Science - Computer Vision and Pattern Recognition},
  file        = {/Users/choekyelnyungmartsang/Zotero/storage/FT9WTHT5/Agnolucci et al. - 2023 - ARNIQA Learning Distortion Manifold for Image Qua.pdf}
}

@online{ImageQX,
  title       = {Explainable {{Image Quality Assessments}} in {{Teledermatological Photography}}},
  author      = {Jalaboi, Raluca and Winther, Ole and Galimzianova, Alfiia},
  date        = {2023-01-23},
  eprint      = {2209.04699},
  eprinttype  = {arxiv},
  eprintclass = {cs},
  url         = {http://arxiv.org/abs/2209.04699},
  urldate     = {2024-04-23},
  abstract    = {Image quality is a crucial factor in the effectiveness and efficiency of teledermatological consultations. However, up to 50\% of images sent by patients have quality issues, thus increasing the time to diagnosis and treatment. An automated, easily deployable, explainable method for assessing image quality is necessary to improve the current teledermatological consultation flow. We introduce ImageQX, a convolutional neural network for image quality assessment with a learning mechanism for identifying the most common poor image quality explanations: bad framing, bad lighting, blur, low resolution, and distance issues. ImageQX was trained on 26,635 photographs and validated on 9,874 photographs, each annotated with image quality labels and poor image quality explanations by up to 12 board-certified dermatologists. The photographic images were taken between 2017 and 2019 using a mobile skin disease tracking application accessible worldwide. Our method achieves expert-level performance for both image quality assessment and poor image quality explanation. For image quality assessment, ImageQX obtains a macro F1-score of 0.73 ± 0.01, which places it within standard deviation of the pairwise inter-rater F1-score of 0.77 ± 0.07. For poor image quality explanations, our method obtains F1-scores of between 0.37 ± 0.01 and 0.70 ± 0.01, similar to the inter-rater pairwise F1-score of between 0.24 ± 0.15 and 0.83 ± 0.06. Moreover, with a size of only 15 MB, ImageQX is easily deployable on mobile devices. With an image quality detection performance similar to that of dermatologists, incorporating ImageQX into the teledermatology flow can enable a better, faster flow for remote consultations.},
  langid      = {english},
  pubstate    = {preprint},
  keywords    = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file        = {/Users/choekyelnyungmartsang/Zotero/storage/5JFM3Q9H/Jalaboi et al. - 2023 - Explainable Image Quality Assessments in Telederma.pdf}
}

@online{TrueImage,
  title       = {{{TrueImage}}: {{A Machine Learning Algorithm}} to {{Improve}} the {{Quality}} of {{Telehealth Photos}}},
  shorttitle  = {{{TrueImage}}},
  author      = {Vodrahalli, Kailas and Daneshjou, Roxana and Novoa, Roberto A. and Chiou, Albert and Ko, Justin M. and Zou, James},
  date        = {2020-10-01},
  eprint      = {2010.02086},
  eprinttype  = {arxiv},
  eprintclass = {cs, eess},
  url         = {http://arxiv.org/abs/2010.02086},
  urldate     = {2024-04-23},
  abstract    = {Telehealth is an increasingly critical component of the health care ecosystem, especially due to the COVID-19 pandemic. Rapid adoption of telehealth has exposed limitations in the existing infrastructure. In this paper, we study and highlight photo quality as a major challenge in the telehealth workflow. We focus on teledermatology, where photo quality is particularly important; the framework proposed here can be generalized to other health domains. For telemedicine, dermatologists request that patients submit images of their lesions for assessment. However, these images are often of insufficient quality to make a clinical diagnosis since patients do not have experience taking clinical photos. A clinician has to manually triage poor quality images and request new images to be submitted, leading to wasted time for both the clinician and the patient. We propose an automated image assessment machine learning pipeline, TrueImage, to detect poor quality dermatology photos and to guide patients in taking better photos. Our experiments indicate that TrueImage can reject 50\% of the sub-par quality images, while retaining 80\% of good quality images patients send in, despite heterogeneity and limitations in the training data. These promising results suggest that our solution is feasible and can improve the quality of teledermatology care.},
  pubstate    = {preprint},
  keywords    = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Computers and Society,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing},
  file        = {/Users/choekyelnyungmartsang/Zotero/storage/ZHAJZ8H3/Vodrahalli et al. - 2020 - TrueImage A Machine Learning Algorithm to Improve.pdf;/Users/choekyelnyungmartsang/Zotero/storage/J95RUXP7/2010.html}
}

@book{DesignThinking,
  title      = {Business {{Innovation}}: {{Das St}}. {{Galler Modell}}},
  shorttitle = {Business {{Innovation}}},
  editor     = {Hoffmann, Christian Pieter and Lennerts, Silke and Schmitz, Christian and Stölzle, Wolfgang and Uebernickel, Falk},
  date       = {2016},
  publisher  = {Springer Fachmedien Wiesbaden},
  location   = {Wiesbaden},
  doi        = {10.1007/978-3-658-07167-7},
  url        = {https://link.springer.com/10.1007/978-3-658-07167-7},
  urldate    = {2024-04-29},
  langid     = {english},
  file       = {/Users/choekyelnyungmartsang/Zotero/storage/EAEZ5T5N/Hoffmann et al. - 2016 - Business Innovation Das St. Galler Modell.pdf}
}
