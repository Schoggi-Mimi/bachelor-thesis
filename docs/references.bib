@online{LIVE,
  title      = {Laboratory for Image and Video Engineering - The University of Texas at Austin},
  url        = {https://live.ece.utexas.edu/research/quality/subjective.htm},
  titleaddon = {{LIVE} Image Quality Assessment Database release 2},
  author     = {Sheikh, Hamid Rahim and Bovik, Alan C. and Cormack, Lawrence and Wang, Zhou},
  urldate    = {2024-04-23},
  file       = {Laboratory for Image and Video Engineering - The University of Texas at Austin:/Users/choekyelnyungmartsang/Zotero/storage/FSBIR66A/subjective.html:text/html}
}

@article{CSIQ,
  title        = {Most apparent distortion: full-reference image quality assessment and the role of strategy},
  volume       = {19},
  issn         = {1017-9909},
  url          = {https://s2.smu.edu/~eclarson/pubs/2010JEI_MAD.pdf},
  doi          = {10.1117/1.3267105},
  shorttitle   = {Most apparent distortion},
  pages        = {011006},
  number       = {1},
  journaltitle = {Journal of Electronic Imaging},
  shortjournal = {J. Electron. Imaging},
  author       = {Chandler, Damon M.},
  urldate      = {2024-04-23},
  date         = {2010-01-01},
  langid       = {english},
  file         = {Chandler - 2010 - Most apparent distortion full-reference image qua.pdf:/Users/choekyelnyungmartsang/Zotero/storage/ND7PFFNY/Chandler - 2010 - Most apparent distortion full-reference image qua.pdf:application/pdf}
}

@article{TID2008,
  title    = {{TID}2008 – A Database for Evaluation of Full- Reference Visual Quality Assessment Metrics},
  abstract = {In this paper, a new image database, {TID}2008, for evaluation of full-reference visual quality assessment metrics is described. It contains 1700 test images (25 reference images, 17 types of distortions for each reference image, 4 different levels of each type of distortion). Mean Opinion Scores ({MOS}) for this database have been obtained as a result of more than 800 experiments. During these tests, observers from three countries (Finland, Italy, and Ukraine) have carried out about 256000 individual human quality judgments. The obtained {MOS} can be used for effective testing of different visual quality metrics as well as for the design of new metrics. Using the designed image database, we have tested several known quality metrics. The designed test image database is freely available for downloading and utilization in scientific investigations.},
  author   = {Ponomarenko, Nikolay and Lukin, Vladimir and Zelensky, Alexander and Egiazarian, Karen and Astola, Jaakko and Carli, Marco and Battisti, Federica},
  langid   = {english},
  file     = {Ponomarenko et al. - TID2008 – A Database for Evaluation of Full- Refer.pdf:/Users/choekyelnyungmartsang/Zotero/storage/M93CPMPI/Ponomarenko et al. - TID2008 – A Database for Evaluation of Full- Refer.pdf:application/pdf}
}

@article{TID2013,
  title        = {Image database {TID}2013: Peculiarities, results and perspectives},
  volume       = {30},
  issn         = {09235965},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S0923596514001490},
  doi          = {10.1016/j.image.2014.10.009},
  shorttitle   = {Image database {TID}2013},
  abstract     = {This paper describes a recently created image database, {TID}2013, intended for evaluation of full-reference visual quality assessment metrics. With respect to {TID}2008, the new database contains a larger number (3000) of test images obtained from 25 reference images, 24 types of distortions for each reference image, and 5 levels for each type of distortion. Motivations for introducing 7 new types of distortions and one additional level of distortions are given; examples of distorted images are presented. Mean opinion scores ({MOS}) for the new database have been collected by performing 985 subjective experiments with volunteers (observers) from five countries (Finland, France, Italy, Ukraine, and {USA}). The availability of {MOS} allows the use of the designed database as a fundamental tool for assessing the effectiveness of visual quality. Furthermore, existing visual quality metrics have been tested with the proposed database and the collected results have been analyzed using rank order correlation coefficients between {MOS} and considered metrics. These correlation indices have been obtained both considering the full set of distorted images and specific image subsets, for highlighting advantages and drawbacks of existing, state of the art, quality metrics. Approaches to thorough performance analysis for a given metric are presented to detect practical situations or distortion types for which this metric is not adequate enough to human perception. The created image database and the collected {MOS} values are freely available for downloading and utilization for scientific purposes.},
  pages        = {57--77},
  journaltitle = {Signal Processing: Image Communication},
  shortjournal = {Signal Processing: Image Communication},
  author       = {Ponomarenko, Nikolay and Jin, Lina and Ieremeiev, Oleg and Lukin, Vladimir and Egiazarian, Karen and Astola, Jaakko and Vozel, Benoit and Chehdi, Kacem and Carli, Marco and Battisti, Federica and Jay Kuo, C.-C.},
  urldate      = {2024-04-23},
  date         = {2015-01},
  langid       = {english},
  file         = {Ponomarenko et al. - 2015 - Image database TID2013 Peculiarities, results and.pdf:/Users/choekyelnyungmartsang/Zotero/storage/NSWLLPSB/Ponomarenko et al. - 2015 - Image database TID2013 Peculiarities, results and.pdf:application/pdf}
}

@article{sheikh_statistical_2006,
  title        = {A Statistical Evaluation of Recent Full Reference Image Quality Assessment Algorithms},
  volume       = {15},
  rights       = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
  issn         = {1057-7149},
  url          = {http://ieeexplore.ieee.org/document/1709988/},
  doi          = {10.1109/TIP.2006.881959},
  abstract     = {Measurement of visual quality is of fundamental importance for numerous image and video processing applications, where the goal of quality assessment ({QA}) algorithms is to automatically assess the quality of images or videos in agreement with human quality judgments. Over the years, many researchers have taken different approaches to the problem and have contributed signiﬁcant research in this area and claim to have made progress in their respective domains. It is important to evaluate the performance of these algorithms in a comparative setting and analyze the strengths and weaknesses of these methods. In this paper, we present results of an extensive subjective quality assessment study in which a total of 779 distorted images were evaluated by about two dozen human subjects. The “ground truth” image quality data obtained from about 25 000 individual human quality judgments is used to evaluate the performance of several prominent full-reference image quality assessment algorithms. To the best of our knowledge, apart from video quality studies conducted by the Video Quality Experts Group, the study presented in this paper is the largest subjective image quality study in the literature in terms of number of images, distortion types, and number of human judgments per image. Moreover, we have made the data from the study freely available to the research community [1]. This would allow other researchers to easily report comparative results in the future.},
  pages        = {3440--3451},
  number       = {11},
  journaltitle = {{IEEE} Transactions on Image Processing},
  shortjournal = {{IEEE} Trans. on Image Process.},
  author       = {Sheikh, H.R. and Sabir, M.F. and Bovik, A.C.},
  urldate      = {2024-04-23},
  date         = {2006-11},
  langid       = {english},
  file         = {Sheikh et al. - 2006 - A Statistical Evaluation of Recent Full Reference .pdf:/Users/choekyelnyungmartsang/Zotero/storage/8F2JWIAS/Sheikh et al. - 2006 - A Statistical Evaluation of Recent Full Reference .pdf:application/pdf}
}

@article{chandler_vsnr_2007,
  title        = {{VSNR}: A Wavelet-Based Visual Signal-to-Noise Ratio for Natural Images},
  volume       = {16},
  rights       = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
  issn         = {1057-7149},
  url          = {http://ieeexplore.ieee.org/document/4286985/},
  doi          = {10.1109/TIP.2007.901820},
  shorttitle   = {{VSNR}},
  abstract     = {This paper presents an efﬁcient metric for quantifying the visual ﬁdelity of natural images based on near-threshold and suprathreshold properties of human vision. The proposed metric, the visual signal-to-noise ratio ({VSNR}), operates via a two-stage approach. In the ﬁrst stage, contrast thresholds for detection of distortions in the presence of natural images are computed via wavelet-based models of visual masking and visual summation in order to determine whether the distortions in the distorted image are visible. If the distortions are below the threshold of detection, the distorted image is deemed to be of perfect visual ﬁdelity ({VSNR} = ) and no further analysis is required. If the distortions are suprathreshold, a second stage is applied which operates based on the low-level visual property of perceived contrast, and the mid-level visual property of global precedence. These two properties are modeled as Euclidean distances in distortion-contrast space of a multiscale wavelet decomposition, and {VSNR} is computed based on a simple linear sum of these distances. The proposed {VSNR} metric is generally competitive with current metrics of visual ﬁdelity; it is efﬁcient both in terms of its low computational complexity and in terms of its low memory requirements; and it operates based on physical luminances and visual angle (rather than on digital pixel values and pixel-based dimensions) to accommodate different viewing conditions.},
  pages        = {2284--2298},
  number       = {9},
  journaltitle = {{IEEE} Transactions on Image Processing},
  shortjournal = {{IEEE} Trans. on Image Process.},
  author       = {Chandler, D.M. and Hemami, S.S.},
  urldate      = {2024-04-23},
  date         = {2007-09},
  langid       = {english},
  file         = {Chandler und Hemami - 2007 - VSNR A Wavelet-Based Visual Signal-to-Noise Ratio.pdf:/Users/choekyelnyungmartsang/Zotero/storage/EW9CANZ4/Chandler und Hemami - 2007 - VSNR A Wavelet-Based Visual Signal-to-Noise Ratio.pdf:application/pdf}
}

@article{ma_waterloo_2017,
  title        = {Waterloo Exploration Database: New Challenges for Image Quality Assessment Models},
  volume       = {26},
  rights       = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
  issn         = {1057-7149, 1941-0042},
  url          = {http://ieeexplore.ieee.org/document/7752930/},
  doi          = {10.1109/TIP.2016.2631888},
  shorttitle   = {Waterloo Exploration Database},
  abstract     = {The great content diversity of real-world digital images poses a grand challenge to image quality assessment ({IQA}) models, which are traditionally designed and validated on a handful of commonly used {IQA} databases with very limited content variation. To test the generalization capability and to facilitate the wide usage of {IQA} techniques in real-world applications, we establish a large-scale database named the Waterloo Exploration Database, which in its current state contains 4744 pristine natural images and 94 880 distorted images created from them. Instead of collecting the mean opinion score for each image via subjective testing, which is extremely difﬁcult if not impossible, we present three alternative test criteria to evaluate the performance of {IQA} models, namely, the pristine/distorted image discriminability test, the listwise ranking consistency test, and the pairwise preference consistency test (P-test). We compare 20 well-known {IQA} models using the proposed criteria, which not only provide a stronger test in a more challenging testing environment for existing models, but also demonstrate the additional beneﬁts of using the proposed database. For example, in the P-test, even for the best performing no-reference {IQA} model, more than 6 million failure cases against the model are “discovered” automatically out of over 1 billion test pairs. Furthermore, we discuss how the new database may be exploited using innovative approaches in the future, to reveal the weaknesses of existing {IQA} models, to provide insights on how to improve the models, and to shed light on how the next-generation {IQA} models may be developed. The database and codes are made publicly available at: https://ece.uwaterloo.ca/{\textasciitilde}k29ma/exploration/.},
  pages        = {1004--1016},
  number       = {2},
  journaltitle = {{IEEE} Transactions on Image Processing},
  shortjournal = {{IEEE} Trans. on Image Process.},
  author       = {Ma, Kede and Duanmu, Zhengfang and Wu, Qingbo and Wang, Zhou and Yong, Hongwei and Li, Hongliang and Zhang, Lei},
  urldate      = {2024-04-23},
  date         = {2017-02},
  langid       = {english},
  file         = {Ma et al. - 2017 - Waterloo Exploration Database New Challenges for .pdf:/Users/choekyelnyungmartsang/Zotero/storage/CZGVWNWD/Ma et al. - 2017 - Waterloo Exploration Database New Challenges for .pdf:application/pdf}
}

@inproceedings{jayaraman_objective_2012,
  location   = {Pacific Grove, {CA}, {USA}},
  title      = {Objective quality assessment of multiply distorted images},
  isbn       = {978-1-4673-5051-8 978-1-4673-5050-1 978-1-4673-5049-5},
  url        = {http://ieeexplore.ieee.org/document/6489321/},
  doi        = {10.1109/ACSSC.2012.6489321},
  abstract   = {Subjective studies have been conducted in the past to obtain human judgments of visual quality on distorted images in or­ der, among other things, to benchmark objective image qual­ ity assessment ({lQA}) algorithms. Existing subjective studies primarily have records of human ratings on images that were corrupted by only one of many possible distortions. However, the majority of images that are available for consumption are corrupted by multiple distortions. Towards broadening the corpora of records of human responses to visual distortions, we recently conducted a study on two types of multiply dis­ torted images to obtain human judgments of the visual qual­ ity of such images. Further, we compared the performance of several existing objective image quality measures on the new database and analyze the effects of multiple distortions on commonly used quality-determinant features and on hu­ man ratings.},
  eventtitle = {2012 46th Asilomar Conference on Signals, Systems and Computers},
  pages      = {1693--1697},
  booktitle  = {2012 Conference Record of the Forty Sixth Asilomar Conference on Signals, Systems and Computers ({ASILOMAR})},
  publisher  = {{IEEE}},
  author     = {Jayaraman, Dinesh and Mittal, Anish and Moorthy, Anush K. and Bovik, Alan C.},
  urldate    = {2024-04-23},
  date       = {2012-11},
  langid     = {english},
  file       = {Jayaraman et al. - 2012 - Objective quality assessment of multiply distorted.pdf:/Users/choekyelnyungmartsang/Zotero/storage/XIL22H3U/Jayaraman et al. - 2012 - Objective quality assessment of multiply distorted.pdf:application/pdf}
}

@article{gu_hybrid_2014,
  title        = {Hybrid No-Reference Quality Metric for Singly and Multiply Distorted Images},
  volume       = {60},
  rights       = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
  issn         = {0018-9316, 1557-9611},
  url          = {https://ieeexplore.ieee.org/document/6879255/},
  doi          = {10.1109/TBC.2014.2344471},
  abstract     = {In a typical image communication system, the visual signal presented to the end users may undergo the steps of acquisition, compression and transmission which cause the artifacts of blurring, quantization and noise. However, the researches of image quality assessment ({IQA}) with multiple distortion types are very limited. In this paper, we ﬁrst introduce a new multiply distorted image database ({MDID}2013), which is composed of 324 images that are simultaneously corrupted by blurring, {JPEG} compression and noise injection. We then propose a new six-step blind metric ({SISBLIM}) for quality assessment of both singly and multiply distorted images. Inspired by the early human visual model and recently revealed free energy based brain theory, our method works to systematically combine the single quality prediction of each emerging distortion type and joint effects of different distortion sources. Comparative studies of the proposed {SISBLIM} with popular full-reference {IQA} approaches and start-of-the-art no-reference {IQA} metrics are conducted on ﬁve singly distorted image databases ({LIVE}, {TID}2008, {CSIQ}, {IVC}, Toyama) and two newly released multiply distorted image databases ({LIVEMD}, {MDID}2013). Experimental results conﬁrm the effectiveness of our blind technique. {MATLAB} codes of the proposed {SISBLIM} algorithm and {MDID}2013 database will be available online at http://gvsp.sjtu.edu.cn/.},
  pages        = {555--567},
  number       = {3},
  journaltitle = {{IEEE} Transactions on Broadcasting},
  shortjournal = {{IEEE} Trans. on Broadcast.},
  author       = {Gu, Ke and Zhai, Guangtao and Yang, Xiaokang and Zhang, Wenjun},
  urldate      = {2024-04-23},
  date         = {2014-09},
  langid       = {english},
  file         = {Gu et al. - 2014 - Hybrid No-Reference Quality Metric for Singly and .pdf:/Users/choekyelnyungmartsang/Zotero/storage/B4SN597A/Gu et al. - 2014 - Hybrid No-Reference Quality Metric for Singly and .pdf:application/pdf}
}

@inproceedings{yang_subjective_2014,
  location   = {Singapore, Singapore},
  title      = {Subjective quality assessment of Screen Content Images},
  isbn       = {978-1-4799-6536-6},
  url        = {http://ieeexplore.ieee.org/document/6982328/},
  doi        = {10.1109/QoMEX.2014.6982328},
  abstract   = {Research on Screen Content Images ({SCIs}) becomes important as they are increasingly used in multi-device communication applications. In this paper, we present a study of subjective quality assessment for distorted {SCIs}, and investigate which part (text or picture) contributes more to the overall visual quality. We construct a large-scale Screen Image Quality Assessment Database ({SIQAD}) consisting of 20 source and 980 distorted {SCIs}. The 11-category Absolute Category Rating ({ACR}) is employed to obtain three subjective quality scores corresponding to the entire image, textual and pictorial regions respectively. Based on the subjective data, we investigate the applicability of 12 state-of-the-art Image Quality Assessment ({IQA}) methods for objectively assessing the quality of {SCIs}. The results indicate that existing {IQA} methods are limited in predicting human quality judgement of {SCIs}. Moreover, we propose a prediction model to account for the correlation between the subjective scores of textual and pictorial regions and the entire image. The current results make an initial move towards objective quality assessment of {SCIs}.},
  eventtitle = {2014 Sixth International Workshop on Quality of Multimedia Experience ({QoMEX})},
  pages      = {257--262},
  booktitle  = {2014 Sixth International Workshop on Quality of Multimedia Experience ({QoMEX})},
  publisher  = {{IEEE}},
  author     = {Yang, Huan and {Yuming Fang} and Lin, Weisi and Wang, Zhou},
  urldate    = {2024-04-23},
  date       = {2014-09},
  langid     = {english},
  file       = {Yang et al. - 2014 - Subjective quality assessment of Screen Content Im.pdf:/Users/choekyelnyungmartsang/Zotero/storage/46AJMFUP/Yang et al. - 2014 - Subjective quality assessment of Screen Content Im.pdf:application/pdf}
}

@article{sun_mdid_2017,
  title        = {{MDID}: A multiply distorted image database for image quality assessment},
  volume       = {61},
  issn         = {00313203},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S0031320316301911},
  doi          = {10.1016/j.patcog.2016.07.033},
  shorttitle   = {{MDID}},
  abstract     = {In this paper, we present a new database, the multiply distorted image database ({MDID}), to evaluate image quality assessment ({IQA}) metrics on multiply distorted images. The database contains 20 reference images and 1600 distorted images. The latter images are obtained by contamination of the former with multiple distortions of random types and levels, so multiple types of distortions appear in each distorted image. Pair comparison sorting ({PCS}) is used as a new subjective rating method to evaluate image quality. This method allows subjects to make equal decisions on images whose difference in quality cannot be easily evaluated visually. A total of 192 subjects participated in the subjective rating, in which mean opinion scores and standard deviations were obtained. In {IQA} research, subjective scores and algorithm predictions are generally related by a nonlinear regression. We further propose a method to initialize the parameters of the nonlinear regression. The experiments of {IQA} metrics conducted on {MDID} validate that this database is advisable and challenging.},
  pages        = {153--168},
  journaltitle = {Pattern Recognition},
  shortjournal = {Pattern Recognition},
  author       = {Sun, Wen and Zhou, Fei and Liao, Qingmin},
  urldate      = {2024-04-23},
  date         = {2017-01},
  langid       = {english},
  file         = {Sun et al. - 2017 - MDID A multiply distorted image database for imag.pdf:/Users/choekyelnyungmartsang/Zotero/storage/JY7KJWKT/Sun et al. - 2017 - MDID A multiply distorted image database for imag.pdf:application/pdf}
}

@article{ni_esim_2017,
  title        = {{ESIM}: Edge Similarity for Screen Content Image Quality Assessment},
  volume       = {26},
  rights       = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
  issn         = {1057-7149, 1941-0042},
  url          = {http://ieeexplore.ieee.org/document/7954714/},
  doi          = {10.1109/TIP.2017.2718185},
  shorttitle   = {{ESIM}},
  abstract     = {In this paper, an accurate full-reference image quality assessment ({IQA}) model developed for assessing screen content images ({SCIs}), called the edge similarity ({ESIM}), is proposed. It is inspired by the fact that the human visual system ({HVS}) is highly sensitive to edges that are often encountered in {SCIs}; therefore, essential edge features are extracted and exploited for conducting {IQA} for the {SCIs}. The key novelty of the proposed {ESIM} lies in the extraction and use of three salient edge features—i.e., edge contrast, edge width, and edge direction. The ﬁrst two attributes are simultaneously generated from the input {SCI} based on a parametric edge model, while the last one is derived directly from the input {SCI}. The extraction of these three features will be performed for the reference {SCI} and the distorted {SCI}, individually. The degree of similarity measured for each above-mentioned edge attribute is then computed independently, followed by combining them together using our proposed edge-width pooling strategy to generate the ﬁnal {ESIM} score. To conduct the performance evaluation of our proposed {ESIM} model, a new and the largest {SCI} database (denoted as {SCID}) is established in our work and made to the public for download. Our database contains 1800 distorted {SCIs} that are generated from 40 reference {SCIs}. For each {SCI}, nine distortion types are investigated, and ﬁve degradation levels are produced for each distortion type. Extensive simulation results have clearly shown that the proposed {ESIM} model is more consistent with the perception of the {HVS} on the evaluation of distorted {SCIs} than the multiple state-of-the-art {IQA} methods.},
  pages        = {4818--4831},
  number       = {10},
  journaltitle = {{IEEE} Transactions on Image Processing},
  shortjournal = {{IEEE} Trans. on Image Process.},
  author       = {Ni, Zhangkai and Ma, Lin and Zeng, Huanqiang and Chen, Jing and Cai, Canhui and Ma, Kai-Kuang},
  urldate      = {2024-04-23},
  date         = {2017-10},
  langid       = {english},
  file         = {Ni et al. - 2017 - ESIM Edge Similarity for Screen Content Image Qua.pdf:/Users/choekyelnyungmartsang/Zotero/storage/KUNRFCHU/Ni et al. - 2017 - ESIM Edge Similarity for Screen Content Image Qua.pdf:application/pdf}
}

@article{min_unified_2017,
  title        = {Unified Blind Quality Assessment of Compressed Natural, Graphic, and Screen Content Images},
  volume       = {26},
  rights       = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
  issn         = {1057-7149, 1941-0042},
  url          = {http://ieeexplore.ieee.org/document/8000398/},
  doi          = {10.1109/TIP.2017.2735192},
  abstract     = {Digital images in the real world are created by a variety of means and have diverse properties. A photographical natural scene image ({NSI}) may exhibit substantially different characteristics from a computer graphic image ({CGI}) or a screen content image ({SCI}). This casts major challenges to objective image quality assessment, for which existing approaches lack effective mechanisms to capture such content type variations, and thus are difﬁcult to generalize from one type to another. To tackle this problem, we ﬁrst construct a cross-content-type ({CCT}) database, which contains 1,320 distorted {NSIs}, {CGIs}, and {SCIs}, compressed using the high efﬁciency video coding ({HEVC}) intra coding method and the screen content compression ({SCC}) extension of {HEVC}. We then carry out a subjective experiment on the database in a well-controlled laboratory environment. Moreover, we propose a uniﬁed content-type adaptive ({UCA}) blind image quality assessment model that is applicable across content types. A key step in {UCA} is to incorporate the variations of human perceptual characteristics in viewing different content types through a multi-scale weighting framework. This leads to superior performance on the constructed {CCT} database. {UCA} is training-free, implying strong generalizability. To verify this, we test {UCA} on other databases containing {JPEG}, {MPEG}-2, H.264, and {HEVC} compressed images/videos, and observe that it consistently achieves competitive performance.},
  pages        = {5462--5474},
  number       = {11},
  journaltitle = {{IEEE} Transactions on Image Processing},
  shortjournal = {{IEEE} Trans. on Image Process.},
  author       = {Min, Xiongkuo and Ma, Kede and Gu, Ke and Zhai, Guangtao and Wang, Zhou and Lin, Weisi},
  urldate      = {2024-04-23},
  date         = {2017-11},
  langid       = {english},
  file         = {Min et al. - 2017 - Unified Blind Quality Assessment of Compressed Nat.pdf:/Users/choekyelnyungmartsang/Zotero/storage/FCJP6NPM/Min et al. - 2017 - Unified Blind Quality Assessment of Compressed Nat.pdf:application/pdf}
}

@article{gu_learning_2020,
  title        = {Learning a Unified Blind Image Quality Metric via On-Line and Off-Line Big Training Instances},
  volume       = {6},
  rights       = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
  issn         = {2332-7790, 2372-2096},
  url          = {https://ieeexplore.ieee.org/document/8627983/},
  doi          = {10.1109/TBDATA.2019.2895605},
  abstract     = {In this work, we resolve a big challenge that most current image quality metrics ({IQMs}) are unavailable across different image contents, especially simultaneously coping with natural scene ({NS}) images or screen content ({SC}) images. By comparison with existing works, this paper deploys on-line and off-line data for proposing a uniﬁed no-reference ({NR}) {IQM}, not only applied to different distortion types and intensities but also to various image contents including classical {NS} images and prevailing {SC} images. Our proposed {NR} {IQM} is developed with two data-driven learning processes following feature extraction, which is based on scene statistic models, free-energy brain principle, and human visual system ({HVS}) characteristics. In the ﬁrst process, the scene statistic models and an image retrieve technique are combined, based on on-line and off-line training instances, to derive a novel loose classiﬁer for retrieving clean images and helping to infer the image content. In the second process, the features extracted by incorporating the inferred image content, free-energy and low-level perceptual characteristics of the {HVS} are learned by utilizing off-line training samples to analyze the distortion types and intensities and thereby to predict the image quality. The two processes mentioned above depend on a gigantic quantity of training data, much exceeding the number of images applied to performance validation, and thus make our model’s performance more reliable. Through extensive experiments, it has been validated that the proposed blind {IQM} is capable of simultaneously inferring the quality of {NS} and {SC} images, and it has attained superior performance as compared with popular and state-of-the-art {IQMs} on the subjective {NS} and {SC} image quality databases. The source code of our model will be released with the publication of the paper at https://kegu.netlify.com.},
  pages        = {780--791},
  number       = {4},
  journaltitle = {{IEEE} Transactions on Big Data},
  shortjournal = {{IEEE} Trans. Big Data},
  author       = {Gu, Ke and Xu, Xin and Qiao, Junfei and Jiang, Qiuping and Lin, Weisi and Thalmann, Daniel},
  urldate      = {2024-04-23},
  date         = {2020-12-01},
  langid       = {english},
  file         = {Gu et al. - 2020 - Learning a Unified Blind Image Quality Metric via .pdf:/Users/choekyelnyungmartsang/Zotero/storage/8K778KAQ/Gu et al. - 2020 - Learning a Unified Blind Image Quality Metric via .pdf:application/pdf}
}

@article{ghadiyaram_massive_2016,
  title        = {Massive Online Crowdsourced Study of Subjective and Objective Picture Quality},
  volume       = {25},
  rights       = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{OAPA}.html},
  issn         = {1057-7149, 1941-0042},
  url          = {http://ieeexplore.ieee.org/document/7327186/},
  doi          = {10.1109/TIP.2015.2500021},
  abstract     = {Most publicly available image quality databases have been created under highly controlled conditions by introducing graded simulated distortions onto high-quality photographs. However, images captured using typical real-world mobile camera devices are usually afﬂicted by complex mixtures of multiple distortions, which are not necessarily well-modeled by the synthetic distortions found in existing databases. The originators of existing legacy databases usually conducted human psychometric studies to obtain statistically meaningful sets of human opinion scores on images in a stringently controlled visual environment, resulting in small data collections relative to other kinds of image analysis databases. Toward overcoming these limitations, we designed and created a new database that we call the {LIVE} In the Wild Image Quality Challenge Database, which contains widely diverse authentic image distortions on a large number of images captured using a representative variety of modern mobile devices. We also designed and implemented a new online crowdsourcing system, which we have used to conduct a very large-scale, multi-month image quality assessment ({IQA}) subjective study. Our database consists of over 350 000 opinion scores on 1162 images evaluated by over 8100 unique human observers. Despite the lack of control over the experimental environments of the numerous study participants, we demonstrate excellent internal consistency of the subjective data set. We also evaluate several top-performing blind {IQA} algorithms on it and present insights on how the mixtures of distortions challenge both end users as well as automatic perceptual quality prediction models. The new database is available for public use at http://live.ece.utexas.edu/research/{ChallengeDB}/index.html.},
  pages        = {372--387},
  number       = {1},
  journaltitle = {{IEEE} Transactions on Image Processing},
  shortjournal = {{IEEE} Trans. on Image Process.},
  author       = {Ghadiyaram, Deepti and Bovik, Alan C.},
  urldate      = {2024-04-23},
  date         = {2016-01},
  langid       = {english},
  file         = {Ghadiyaram und Bovik - 2016 - Massive Online Crowdsourced Study of Subjective an.pdf:/Users/choekyelnyungmartsang/Zotero/storage/NQWGF6QQ/Ghadiyaram und Bovik - 2016 - Massive Online Crowdsourced Study of Subjective an.pdf:application/pdf}
}

@article{virtanen_cid2013_2015,
  title        = {{CID}2013: A Database for Evaluating No-Reference Image Quality Assessment Algorithms},
  volume       = {24},
  rights       = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{OAPA}.html},
  issn         = {1057-7149, 1941-0042},
  url          = {http://ieeexplore.ieee.org/document/6975172/},
  doi          = {10.1109/TIP.2014.2378061},
  shorttitle   = {{CID}2013},
  abstract     = {This paper presents a new database, {CID}2013, to address the issue of using no-reference ({NR}) image quality assessment algorithms on images with multiple distortions. Current {NR} algorithms struggle to handle images with many concurrent distortion types, such as real photographic images captured by different digital cameras. The database consists of six image sets; on average, 30 subjects have evaluated 12–14 devices depicting eight different scenes for a total of 79 different cameras, 480 images, and 188 subjects (67\% female). The subjective evaluation method was a hybrid absolute category rating-pair comparison developed for the study and presented in this paper. This method utilizes a slideshow of all images within a scene to allow the test images to work as references to each other. In addition to mean opinion score value, the images are also rated using sharpness, graininess, lightness, and color saturation scales. The {CID}2013 database contains images used in the experiments with the full subjective data plus extensive background information from the subjects. The database is made freely available for the research community.},
  pages        = {390--402},
  number       = {1},
  journaltitle = {{IEEE} Transactions on Image Processing},
  shortjournal = {{IEEE} Trans. on Image Process.},
  author       = {Virtanen, Toni and Nuutinen, Mikko and Vaahteranoksa, Mikko and Oittinen, Pirkko and Hakkinen, Jukka},
  urldate      = {2024-04-23},
  date         = {2015-01},
  langid       = {english},
  file         = {Virtanen et al. - 2015 - CID2013 A Database for Evaluating No-Reference Im.pdf:/Users/choekyelnyungmartsang/Zotero/storage/KHXSACRZ/Virtanen et al. - 2015 - CID2013 A Database for Evaluating No-Reference Im.pdf:application/pdf}
}
