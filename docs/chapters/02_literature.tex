\chapter{Literature Review}
\label{ch:LiteratureReview}

\section{Image Evaluation}
\label{sec:ImageEvaluation}
There are three ways to evaluate an image: assessing its quality, aesthetics, or fidelity \autocite{D}. Each method focuses on different aspects of image evaluation and has unique applications. \par
\vspace{\baselineskip}
\noindent
\textbf{Image Quality Assessment (IQA)} measures the degradation of an image. This involves comparing an original, undistorted image with a processed version that has undergone changes such as compression, noise addition, or artifact introduction. The goal is to quantify how much the image quality has declined due to these changes.\par
\vspace{\baselineskip}
\noindent
\textbf{Image Aesthetics Assessment} focuses on the visual appeal of an image. It evaluates how pleasing an image is to the human eye, considering factors like composition, color, and overall aesthetic impact. While related to IQA, since both involve human judgment, this area is not the focus of this thesis because it deals more with subjective perceptions of beauty rather than measurable quality degradations. \par
\vspace{\baselineskip}
\noindent
\textbf{Image Fidelity Assessment} deals with how accurately an image represents the original scene or view. This is especially relevant in applications involving multiple views or stereo cameras, assessing the correctness of image reconstruction. However, this thesis will also not cover image fidelity assessment, as it relates more to the accuracy of recreating an image rather than evaluating its quality after processing. \par
\vspace{\baselineskip}
\noindent
The primary focus of this thesis is on image quality assessment, specifically looking at different types of image degradation. \par
\clearpage
\subsection{Methods of Image Quality Assessment}
\label{sub:MethodsImageQualityAssessment}
There are two main approaches to assess image quality: subjective and objective methods \autocite{D}. \par
\subsubsection{Subjective Quality Assessment}
\label{subsub:SubjectiveQualityAssessment}
Subjective quality assessment involves human observers evaluating the quality of images based on their visual perception. This method is essential for understanding how humans perceive image quality in real-world situations, especially when technical measurements might not fully capture what people actually see and experience. There are two primary methods used in subjective quality assessment:\par
\begin{itemize}
    \item Absolute Categorical Rating:   In this approach, human observers are presented with an unlabeled image and asked to rate its quality based on predefined categories. Each observer evaluates the image independently, without comparing it to any reference image. This method allows evaluators to provide a direct judgment on the image’s quality based on their subjective experience.
    \item Paired Comparison:  In this method, human observers are presented with two images: an unlabeled image and a reference image. Observers then assess the quality of the unlabeled image by comparing it directly to the reference image, assigning a score based on the perceived differences in quality.
\end{itemize}
Subjective quality assessment is highly valued for its ability to accurately reflect human perception of image quality \autocite{D}. However, this method is also resource-intensive, requiring significant time and effort from human evaluators. Additionally, subjective assessments can be influenced by variability and biases introduced by individual scorers. For example, differences in monitor color calibration, domain knowledge, and personal preferences can affect the consistency and reliability of the evaluations. Despite these challenges, subjective quality assessment remains a critical component of comprehensive image quality evaluation, particularly in applications where how a human perceives the image is the final measure of its quality. \par

\subsubsection{Objective Quality Assessment}
\label{subsub:ObjectiveQualityAssessment}
Objective quality assessment uses mathematical algorithms to evaluate image quality instead of relying on human judgment. These algorithms are often based on our understanding of how the human vision system works. However, not all methods directly simulate human vision. Some methods use different techniques to measure quality by comparing specific features or data points extracted from images. This type of assessment is mainly categorized into three methods based on the amount of reference data used: Full-Reference IQA (FR-IQA), Reduced-Reference IQA (RR-IQA), and No-Reference IQA (NR-IQA) \autocite{D}. \par
\vspace{\baselineskip}
\noindent
\textbf{Full-Reference IQA} (FR-IQA) involves a detailed comparison between a distorted image and a reference image (see \autoref{fig:FRIQA}). Features are extracted from both images, and their differences are quantitatively analyzed to compute a quality score. While FR-IQA offers detailed assessments, it requires a reference image for every distorted image evaluated, which can limit its practicality. \par
\clearpage
\begin{figure}[ht]
    \centering
    \includegraphics[keepaspectratio,width=12cm]{img/FRIQA.jpg}
    \caption{General framework of FR-IQA algorithms. Features are extracted from both images, and then the feature distance is calculated.}
    \label{fig:FRIQA}
\end{figure}
\noindent
\textbf{Reduced-Reference IQA} (RR-IQA) operates similarly to FR-IQA but does not need the complete reference image. Instead, it uses a reduced set of features extracted from both the distorted and reference images (see \autoref{fig:RRIQA}). This method balances the detailed comparison of FR-IQA with the independence of NR-IQA (which will be discussed later), reducing computational requirements while still providing meaningful quality assessments based on partial reference data. \par
\begin{figure}[ht]
    \centering
    \includegraphics[keepaspectratio,width=12cm]{img/RRIQA.jpg}
    \caption{General framework of RR-IQA algorithms. Features of the reference and distorted images are extracted and used collectively to compute the quality.}
    \label{fig:RRIQA}
\end{figure}
\noindent
Both FR-IQA and RR-IQA use two methods to analyze image quality \autocite{D}:
\begin{itemize}
    \item Spatial-Based Analysis: This method compares images pixel by pixel or region by region. It offers straightforward interpretation and efficient computation. However, it may not fully align with how humans process images and can lack robustness in some scenarios.
    \item Transform-Based Analysis: This approach transforms images into a different domain (such as the frequency domain) that more closely mimics how humans process images. While this method is robust, it is complex and computationally intensive.
\end{itemize}
\vspace{\baselineskip}
\noindent
\textbf{No-Reference IQA} (NR-IQA) does not rely on any reference image. Instead, it analyzes the distorted image alone by extracting features indicative of quality (see \autoref{fig:NRIQA}). This method is particularly useful when no reference images are available, such as in many practical applications of teledermatology. NR-IQA can be customized to target specific types of distortions or created for general-purpose quality assessment, making it adaptable for different fields. \par
\begin{figure}[ht]
    \centering
    \includegraphics[keepaspectratio,width=12cm]{img/NRIQA.jpg}
    \caption{General framework of NR-IQA algorithms.}
    \label{fig:NRIQA}
\end{figure}
\clearpage
\noindent
For this thesis, the focus will be on no-reference image quality assessment because it is especially relevant for evaluating teledermatology images where reference images are usually not available. Since IQA measures distortions and NR-IQA can handle various types, it is important to identify the most common distortions encountered. The next subsection will discuss these distortions in detail. \par

\subsection{Common Distortions in Image Quality Assessment}
\label{sub:CommonDistortionsIQA}
IQA must address various distortions that can significantly affect the perceived quality of images. Understanding these common distortions is crucial for developing effective IQA algorithms, particularly in contexts like teledermatology, where accurate image assessment is critical. \autoref{fig:distortions} shows the common distortions typically considered in IQA, with a reference image shown first for better comparison \autocite{ARNIQA}. \par
\vspace{\baselineskip}
\noindent
The common distortions are:
\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{img/Original.jpg}
        \caption{Original}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{img/Blur.jpg}
        \caption{Blur}
        \label{fig:blur}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{img/Sharpness.jpg}
        \caption{Sharpness}
        \label{fig:sharpness}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{img/Noise.jpg}
        \caption{Noise}
        \label{fig:noise}
    \end{subfigure} 

    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{img/ColorAccuracy.jpg}
        \caption{Color Accuracy}
        \label{fig:color_accuracy}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{img/BrightnessContrast.jpg}
        \caption{Brightness \& Contrast}
        \label{fig:brightness_contrast}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{img/Artifacts.jpg}
        \caption{Artifacts}
        \label{fig:artifacts}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{img/Compression.jpg}
        \caption{Compression}
        \label{fig:compression}
    \end{subfigure}
    \caption{Examples of Common Distortions in Images. (adapted from \autocite{ARNIQA})}
    \label{fig:distortions}
\end{figure}
\begin{enumerate}
    \item \textbf{Blur}: Blurred images lack sharpness and clarity, often resulting from motion during capturing, incorrect focus settings, or imperfections in the camera lens. (See \autoref{fig:blur} for an example of a blurred image.)
    \item \textbf{Sharpness}: Sharpness refers to how well-defined the edges and fine details in an image appear. High sharpness indicates clear, crisp images, while low sharpness makes an image look soft and unclear. (See \autoref{fig:sharpness} for an example of a sharpened image.)
    \item \textbf{Noise}: Noise appears as random variations in brightness or color and is often due to the limitations of the camera’s sensor, particularly under low light conditions or at high ISO settings. (See \autoref{fig:noise} for an example of a noisy image.)
    \item \textbf{Color Accuracy}:  Color accuracy refers to how faithfully colors are reproduced in an image. Distortions in color accuracy can lead to inaccurate or unrealistic color representation. (See \autoref{fig:color_accuracy} for an example of a color distorted image.)
    \item \textbf{Brightness \& Contrast}: Brightness is the overall light level of an image, while contrast refers to the range between its darkest and lightest areas. Proper balance of both is crucial for maintaining image visibility and detail. Excessive or insufficient brightness and contrast can make an image unusable for detailed analysis. (See \autoref{fig:brightness_contrast} for an example of an image with altered brightness.)
    \item \textbf{Artifacts}: Artifacts are unwanted visual anomalies introduced during image acquisition or processing, such as halos, or jagged edges. (See \autoref{fig:artifacts} for an example of an image with artifacts.)
    \item \textbf{Compression}: When images are compressed to reduce file size, this often results in lost detail and visible quality degradation. (See \autoref{fig:compression} for an example of a compressed image.)
\end{enumerate}
Each type of distortion affects the visual quality and perceived accuracy of images, influencing the effectiveness of IQA methodologies in assessing image quality. Understanding these distortions is essential for developing robust quality assessment algorithms and improving image clarity in different applications, including teledermatology. \par

\subsection{Benchmark Datasets for Image Quality Assessment}
\label{sub:BenchmarkDatasetsIQA}
Benchmark datasets play an important role in advancing IQA. They provide standardized and diverse image sets with known distortions and corresponding quality annotations, which researchers use to evaluate and improve IQA algorithms. These annotations, often in the form of Mean Opinion Score (MOS) and Differential Mean Opinion Score (DMOS),  are used to assess image quality \autocite{D}. \par
\vspace{\baselineskip}
\noindent
\textbf{Mean Opinion Score (MOS)} is calculated by averaging ratings from human observers who judge the quality of images on a predefined scale. This score reflects the overall perceptual quality as seen by typical viewers and is widely used to compare the performance of different IQA methods against human visual judgment. \par
\vspace{\baselineskip}
\noindent
\textbf{Differential Mean Opinion Score (DMOS)}, on the other hand, is derived from MOS and measures the perceived difference in quality between a reference image and a distorted version. This score is particularly useful for understanding the impact of specific distortions on image quality. \par
\vspace{\baselineskip}
\noindent
An overview of IQA databases is provided in \autoref{tab:iqa_databases}, and more detailed descriptions can be found in \autoref{sec:Dataset}. These datasets enable researchers to thoroughly test the robustness, accuracy, and generalization capabilities of different IQA methods. They also help in developing new algorithms by providing reliable quality scores, which are essential for ensuring reproducibility and consistency in research. \par

\begin{table*}[!t]
    \centering
    \caption{An overview of IQA databases}
    \label{tab:iqa_databases}
    \renewcommand{\arraystretch}{1.5}
    \resizebox{\textwidth}{!}{ % Resize table to fit within the text width
    \begin{tabular}{p{2.3cm} p{2cm} c c c p{3.5cm} c  p{2.3cm} c}
    \toprule
        Category & Database & Year & \#Ref. & \#Dist. & \#Dist. Type & \#Dist. Level & Resolution Type & Ground-truth \\
        \hline
        \multirow{8}{*}{General} 
        & LIVE & 2004 & 30 & 779 & JPEG, JP2K, WN, GB, FF& 5 or 4 & 768 $\times$ 512 & DMOS \\
        & TID2008 & 2008 & 25 & 1700 & 17 \footnote{See detailed types on database page: \url{https://www.ponomarenko.info/tid2008.htm}} & 4 & 512 $\times$ 384 & MOS \\
        & TID2013 & 2013 & 25 & 3000 & 24 \footnote{See detailed types on database page: \url{https://www.ponomarenko.info/tid2013.htm}} & 5 & 512 $\times$ 384 & MOS \\
        & CSIQ & 2009 & 30 & 866 & JPEG, JP2K, WN, GB, APGN,  GCD & 5 or 4 & 512 $\times$ 512 & DMOS \\
        & A57 & 2007 & 3 & 54 & DWT, AGWN, JPEG, JP2K, JP2K-DCQ, GB& 3 & 512 $\times$ 512 & MOS \\
        & WED & 2017 & 4744 & 94880 & JPEG, JP2K, GB, WN& 5 & - & - \\
        & KADID-10k & 2019& 81& 10125& 25 \footnote{See detailed types on database page: \url{https://database.mmsp-kn.de/kadid-10k-database.html}} & 5& 512 $\times$ 384 & DMOS\\
        & KADIS-700k & 2020& 140000& 700000& 25 \footnote{See detailed types on database page: \url{https://database.mmsp-kn.de/kadid-10k-database.html}} & 5& 512 $\times$ 384 & DMOS\\
        \hline
        \multirow{3}{*}{Multiple Dist.} 
        & LIVEMD & 2012 & 15 & 405 & GB followed by JPEG, GB followed by WN& - & 1280 $\times$ 720 & DMOS \\
        & MDID2013 & 2013 & 12 & 324 & corrupted successively by GB, WN, and JPEG& - & 768 $\times$ 512 or 1280 $\times$ 720 & DMOS \\
        & MDID2016 & 2016 & 20 & 1600 & GB or CC ﬁrst, JPEG or JP2K second and WN last& - & 512 $\times$ 384 & MOS \\
        \hline
        \multirow{4}{*}{Screen content} 
        & SIQAD & 2014 & 20 & 980 & WN, GB, CC, JPEG, JP2K, MB, LSBC& 7 & 700 $\times$ 700 & DMOS \\
        & SCIQ & 2017 & 40 & 1800 & WN, GB, MB, CC, JPEG, JP2K, CSC, CQD& 5 & 1280 $\times$ 720 & MOS \\
        & CCT & 2017 & 72 & 1320 & HEVC and HEVC-SCC coding& 11 & 1280 $\times$ 720 to 1920 $\times$ 1080 & MOS \\
        & HSNID & 2019 & 20 & 600 & WN, GB, MB, CC, JPEG, JP2K& 5 &  - & MOS \\
        \hline
        \multirow{2}{*}{Authentic Dist.} 
        & LIVE Wild & 2016 & 0 & 1162 & - & - & 500 $\times$ 500 & MOS \\
        & CID2013 & 2015 & 0 & 480 & - & - & 1600 $\times$ 1200 & MOS \\
    \bottomrule
    \end{tabular}
    }
    \raggedright
    \scalebox{0.59}{
    \begin{tabular}{l}
        Note: \#Ref.: Total number of pristine images. \quad \#Dist.: Total number of distorted images. \quad AGWN: Additive Gaussian white noise. \quad WN: White noise.\\
        \quad  \quad  \quad APGN: Additive pink Gaussian noise. \quad CC: Contrast change. \quad CSC: Color saturation change. \quad CQD: Color quantization with dithering.\\
        \quad  \quad  \quad DWT: Quantization of the LH subbands of a 5-level DWT. \quad FF: Simulated fast fading Rayleigh channel. \quad GB: Gaussian blur. \quad MB: Motion blur.\\
        \quad  \quad  \quad GCD: Global contrast decrements. \quad HEVC-SCC: Screen content coding extension of high efficiency video coding. \quad JPEG: JPEG compression. \\
        \quad  \quad  \quad JP2K: JPEG2000 compression. \quad JP2K-DCQ: JPEG-2000 compression with DCQ. \quad LSBC: Layer segmentation based compression.
    \end{tabular}
    }
\end{table*}


\subsection{State-of-the-Art in Image Quality Assessment}
\label{sub:SOTA_IQA}
The current state-of-the-art in IQA is ARNIQA \autocite{ARNIQA}, with version 2 released in late 2023. ARNIQA, which stands for “leArning distoRtion maNifold for Image Quality Assessment,” represents a significant advancement in No-Reference Image Quality Assessment (NR-IQA). This technology aims to measure image quality based on human perception, even without a reference image. \par
\vspace{\baselineskip}
\vspace{\baselineskip}
\vspace{\baselineskip}
\vspace{\baselineskip}

\begin{figure}[ht]
    \centering
    \includegraphics[keepaspectratio,width=15cm]{img/method_SimCLR.jpg}
    \caption{Overview of the training strategy for ARNIQA. Two pristine images are cropped and equally degraded. The model maximizes the similarity of their embeddings while minimizing the similarity to embeddings from degraded crops of half-scale versions of the original images. This process creates hard negative examples by introducing downsample distortion, demonstrating how original and half-scale degraded crops differ despite identical degradation. \autocite{ARNIQA}.}
    \label{fig:SimCLR}
\end{figure}
\noindent
ARNIQA is developed using a self-supervised learning approach, allowing it to learn a comprehensive model of different image distortions. Instead of focusing on the content of the images, ARNIQA focuses on understanding the types and qualities of distortions. This characteristic makes it highly adaptable across different domains where image content can vary significantly. \par
\vspace{\baselineskip}
\vspace{\baselineskip}
\noindent
One of the key strengths of ARNIQA is its ability to synthetically degrade images through up to 1.9 billion distinct degradation patterns. It can apply up to seven different types of distortions simultaneously, covering a wide range of real-world scenarios. This extensive training with a wide variety of distortions ensures that ARNIQA can accurately assess image quality in different conditions and avoid the need for large labeled datasets. \par
\vspace{\baselineskip}
\noindent
At the core of ARNIQA is the SimCLR (Simple Framework for Contrastive Learning) framework. This framework helps ARNIQA learn meaningful representations of image quality by comparing different versions of the same image and focusing on their similarities and differences. By constructing positive pairs through the application of the same distortion settings to different images, SimCLR makes sure the model focuses on the distortions rather than the content. To improve the model’s ability to distinguish between different types of distortions, SimCLR introduces slight variations by downsampling images before cropping and applying distortions, creating hard negative examples. These examples help the model differentiate between similar-looking images with different types of degradation, thereby improving its ability to provide accurate image quality assessments (see \autoref{fig:SimCLR}). \par
\vspace{\baselineskip}
\noindent
A linear regressor is then used to map the features extracted from the backbone to a quality score ranging from 0 to 1. This score reflects the relative quality of the image based on the distortions.\par
\vspace{\baselineskip}
\noindent
ARNIQA achieves high performance with only up to 0.5\% of the data needed for training compared to other state-of-the-art methods because it focuses on distortion patterns rather than image content. It provides reliable and consistent quality assessments across a wide range of distortions and severities, showing its robustness. This makes ARNIQA particularly suitable for teledermatology, as it can handle different image quality resulting from different lighting conditions, camera quality, and patient handling \autocite{ARNIQA}. \par
\vspace{\baselineskip}
\noindent
By leveraging the feature extraction backbone from ARNIQA, this thesis aims to use its advanced capabilities to enhance the assessment of image quality in teledermatology. \par

\section{Teledermatology}
\label{sec:Teledermatology}
This section explains what teledermatology is and discusses why having high-quality images is crucial for accurate diagnoses and treatment. It reviews the quality standards needed for teledermatology images, along with public datasets available for research. Additionally, it examines different methods used to assess image quality in teledermatology based on previous studies. In the final section, the challenges and opportunities in the field are explored, focusing on how to improve image quality assessment. This approach helps in understanding the current state of teledermatology and finding ways to enhance it. \par

\subsection{Introduction to Teledermatology}
\label{sub:IntroductionTeledermatology}
Teledermatology is a branch of telemedicine that allows dermatologists to provide remote consultations and treatments using telecommunications technology. This is especially beneficial for patients in remote areas, making sure they receive timely and effective skin care. There are two main methods used in teledermatology: real-time (synchronous) and store-and-forward (asynchronous) \autocite{SaF}.\par
\vspace{\baselineskip}
\noindent
\textbf{Real-time} teledermatology involves live video consultations between the dermatologist and the patient. It allows for immediate interaction and feedback, making it useful for urgent cases. However, it requires both the patient and the dermatologist to be available at the same time, which can be a limitation. \par
\vspace{\baselineskip}
\noindent
\textbf{Store-and-forward} teledermatology involves sending medical information, including images and patient history, to dermatologists who review it later. This approach offers more flexibility since it doesn’t require the patient and dermatologist to be available simultaneously \autocite{SaF}. \par
\vspace{\baselineskip}
\noindent
Given that store-and-forward teledermatology is the focus, it is important to note that high-quality images are essential in this method. The accuracy of remote diagnoses depends directly on the quality of the images provided. Poor image quality can lead to incorrect diagnoses or delayed treatment, thereby reducing the effectiveness of teledermatology. Therefore, ensuring that images meet specific quality standards is crucial for the success of teledermatology services. \par

\subsection{Quality Criteria for Teledermatology Images}
\label{sub:QualityCriteriaTeledermatology}
The International Skin Imaging Collaboration (ISIC) has set guidelines to standardize images based on lighting, background color, field of view for dermoscopic images, image orientation, focus and depth of field, resolution, scale and measurement, color calibration, and image storage. This thesis will focus on seven key criteria from the original nine guidelines that directly impact image quality. The other two criteria, scale and measurement and image storage, are excluded as they do not directly affect image quality. Scale and Measurement are less important here as it involves providing a reference for size within the image, which is not crucial for quality assessment. Image Storage deals more with regulations than with image quality \autocite{TDCriteria}. \par
\vspace{\baselineskip}
\noindent
Throughout this thesis, the term “distortion” refers to any changes or issues with the seven dermatology quality criteria. This includes problems with lighting, background, field of view, orientation, focus, resolution, and color calibration. These seven key criteria for teledermatology images, along with recommendations on how to meet each criterion, are as follows: \par
\clearpage
\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{img/Reference.jpg}
        \caption{Good Quality}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{img/Lighting.jpg}
        \caption{Lighting}
        \label{fig:lighting}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{img/Background.jpg}
        \caption{Background}
        \label{fig:background}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{img/FoV.jpg}
        \caption{Field of View}
        \label{fig:FoV}
    \end{subfigure} 

    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{img/Orientation.jpg}
        \caption{Orientation}
        \label{fig:orientation}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{img/Focus.jpg}
        \caption{Focus}
        \label{fig:focus}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{img/Resolution.jpg}
        \caption{Resolution}
        \label{fig:resol}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{img/Color.jpg}
        \caption{Color Calibration}
        \label{fig:cc}
    \end{subfigure}
    \caption{Examples of teledermatology images showing good quality, poor lighting, distracting background, improper field of view, incorrect orientation, lack of focus, low resolution, and poor color calibration.}
    \label{fig:quality_criteria}
\end{figure}
\begin{enumerate}
    \item \textbf{Lighting}:  Good lighting is essential. It should be even and not too harsh, avoiding shadows or bright spots that can hide details. \textit{Use natural light or soft artificial light to clearly show the skin lesion.}
    \item \textbf{Background}:  The background should be plain and uncluttered to keep the focus on the skin issue. A simple, non-reflective background like white or gray works best. \textit{Use a plain, non-reflective background to minimize distractions and keep the focus on the skin lesion.}
    \item \textbf{Field of View}: The image should include the entire lesion and some surrounding skin. This helps provide context for a more accurate diagnosis. \textit{Make sure the lesion is centered and fully visible in the frame.}
    \item \textbf{Orientation}: The image should be taken from the correct angle to match standard anatomical positions. This helps the dermatologist understand where the lesion is on the body. \textit{Keep the camera straight and aligned with the lesion.}
    \item \textbf{Focus \& Depth of Field}: The image should be in sharp focus, with the entire lesion clear and detailed. Adjust the camera settings to ensure the lesion is not blurry. \textit{Make sure the camera is in focus and adjust the aperture to achieve sufficient depth of field.}
    \item \textbf{Resolution}: High resolution is important to show fine details. Use a camera with good resolution settings to capture clear and detailed images of the skin. \textit{Adjust the camera settings to the highest resolution possible to ensure clarity and precision in the image.}
    \item \textbf{Color Calibration}:  Accurate colors are necessary to assess the skin lesion correctly. Make sure the colors in the image match real life. \textit{Use a color reference chart or adjust the white balance settings on the camera to ensure accurate color reproduction.}
\end{enumerate}
By following these guidelines, patients can make sure their images are of high quality, leading to better diagnoses and patient care. \par


\subsection{Teledermatology Datasets}
\label{sub:DatasetsTD}
In teledermatology, having high-quality image datasets is crucial for developing and testing methods to assess image quality. While many datasets exist for dermatology, they are not always designed specifically for teledermatology. Dermatology datasets often include professional images taken in clinical settings, such as close-up dermoscopic images that provide detailed views of the skin. In contrast, teledermatology images might be taken by patients using their mobile devices, resulting in more inconsistent quality. However, several datasets can still be useful for teledermatology, depending on the specific use case. Here are four public datasets that can be used for teledermatology: \par
\begin{itemize}
    \item \textbf{DDI} \autocite{DDI}: This dataset provides 656 high-quality images curated by dermatologists for detailed skin tone evaluation and diagnostic accuracy.
    \item \textbf{Fitzpatrick17k} \autocite{F17K}: This dataset includes 16'577 images annotated for Fitzpatrick skin type \autocite{Fitzpatrick} across 114 different skin conditions. It is valuable for studying a wide range of skin conditions and their presentations across different skin types.
    \item \textbf{Monkeypox Dataset 2022} \autocite{Monkeypox}: This dataset contains approximately 1'905 images focused on monkeypox, useful for developing diagnostic tools.
    \item \textbf{SCIN} \autocite{SCIN}: The Skin Condition Image Network emerges from a crowdsourcing initiative. This dataset contains 10'408 images capturing a broad spectrum of dermatological conditions. Unlike many dermatology datasets that mainly focus on skin cancer diagnostics by classifying malignant and benign tumors, the SCIN dataset covers a broader range of common dermatological conditions, including allergic, inflammatory, and infectious diseases. These conditions are frequently encountered in everyday clinical practice but are underrepresented in existing datasets. The SCIN dataset is particularly valuable because it captures images of early-stage conditions. Over half of the images were taken less than a week from the onset of symptoms, with 30\% captured less than a day after symptoms appeared. This dataset includes conditions that patients are likely to consult about via teledermatology platforms before visiting traditional healthcare settings.
\end{itemize}
\noindent
For this thesis, the Fitzpatrick17k and SCIN datasets will be particularly important. These datasets provide extensive and diverse image collections that will be used to develop and test image quality assessment methods in teledermatology. The Fitzpatrick17k dataset contains more clinical setting images, which provide good quality but do not represent the variability seen in typical teledermatology images \autocite{F17K}. Therefore, the Fitzpatrick17k dataset will be used primarily for training purposes to complement the SCIN dataset, which better represents the real-world scenarios encountered in teledermatology. \par 
\clearpage
\subsection{Related Work on Image Quality Assessment in Teledermatology}
\label{sub:ApproachesIQAinTeledermatology}
In teledermatology, two key methods for detecting image quality have been highlighted in previous studies: TrueImage \autocite{TrueImage} and ImageQX \autocite{ImageQX}. Both methods work closely with dermatologists to ensure their models understand what is needed for accurate diagnoses. \par
\vspace{\baselineskip}
\noindent
\textbf{TrueImage} (A Machine Learning Algorithm to Improve the Quality of Telehealth Photos), introduced in 2020, uses an automated machine learning system to detect poor-quality dermatology images and help patients take better images. TrueImage uses a semantic segmentation algorithm to identify skin areas, then generates features and classifies the quality. It focuses on common issues like blur, poor lighting, and zoom problems. TrueImage is efficient enough to run on older smartphones and is easy to understand, making it reliable across different skin tones. It was trained on a diverse dataset, including images from Google Images and Stanford Health Care. However, it has limitations: it cannot handle cases where only the background is blurry or poorly lit, it cannot detect framing issues (problems with how the image is arranged, such as when the skin area is not centered or properly aligned), and it cannot discard images that do not contain skin \autocite{TrueImage}. \par
\vspace{\baselineskip}
\noindent
\textbf{ImageQX} (Explainable Image Quality Assessments in Teledermatological Photography), released in January 2023, is a convolutional neural network that automatically assesses the quality of dermatology images. It focuses on issues like bad framing, poor lighting, blur, low resolution, and distance problems. ImageQX was trained on 26'635 images and validated on 9'c874 images, each annotated by up to 12 board-certified dermatologists. Its main innovation is providing explanations for poor quality and guiding patients on how to take better images. ImageQX is also lightweight, only 15 MB, and can be easily used on mobile devices. It achieves a macro F1-score of 0.73, showing its effectiveness in real-world applications. However, it has limitations in handling certain quality issues, like explaining blurry images, and relies heavily on dermatologist-annotated images, highlighting the need for a diverse and high-quality training dataset \autocite{ImageQX}.
\par
\vspace{\baselineskip}
\noindent
Both ImageQX and TrueImage make significant contributions to automated image quality assessment in teledermatology. TrueImage primarily focuses on issues like focus, lighting, and field of view, while ImageQX addresses additional factors such as resolution and distance (e.g., if the skin is too far away in the image). Both methods perform well on detecting blur and lighting issues but have room for improvement in handling field of view, resolution, and distance. Specifically, ImageQX has F1-scores of 0.37 for field of view, 0.61 for lighting, 0.70 for focus, 0.52 for resolution, and 0.42 for distance \autocite{ImageQX}. From these methods, it is clear that lightweight models, providing actionable feedback to users, and using a diverse training dataset to ensure robustness are important. \par
\clearpage
\section{Challenges and Opportunities in Image Quality Assessment for Teledermatology}
\label{sec:ChallengesOpportunitiesTeledermatology}
\vspace{\baselineskip}

\subsection{Challenges}
One major challenge in IQA is that evaluating image quality can be very subjective. This means that different people might have different opinions on what makes an image look good or bad. This subjectivity makes it hard to create standard measures that everyone agrees on. In teledermatology, where image quality directly affects medical diagnoses, this becomes a significant issue \autocite{D}. \par
\vspace{\baselineskip}
\noindent
Another challenge is the variety of problems that can affect image quality. In teledermatology specifically, patients use different devices under different conditions, leading to inconsistencies in image quality. This variability adds another layer of complexity to developing effective IQA methods. Additionally, in many real-world applications, we often do not have high-quality reference images to compare against. This absence makes it difficult to evaluate the quality of images taken by patients. Therefore, developing methods that do not need reference images (No-Reference IQA or NR-IQA) is essential. \par

\subsection{Opportunities}
Despite these challenges, there are significant opportunities to improve IQA in teledermatology. One promising area is the advancement of self-supervised learning techniques. These methods, like those used in ARNIQA \autocite{ARNIQA}, allow models to learn from large amounts of data without needing extensive labeled examples. This approach saves time and money because it reduces the need for manually labeled data. It also enables the development of high-quality IQA models that can work well even without reference images. \par
\vspace{\baselineskip}
\noindent
Another opportunity lies in collaborating closely with dermatologists. Methods like ImageQX \autocite{ImageQX} and TrueImage \autocite{TrueImage} have shown the benefits of such collaboration, ensuring that IQA models meet the specific needs of medical professionals. These models can provide real-time, useful guidance to patients on how to take better images, thereby improving the quality of images submitted for remote consultations. \par
